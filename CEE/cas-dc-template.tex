\documentclass[a4paper,fleqn]{cas-dc}
%\documentclass[a4paper,fleqn,longmktitle]{cas-dc}
\usepackage[numbers]{natbib}
%\usepackage[authoryear]{natbib}
%\usepackage[authoryear,longnamesfirst]{natbib}
%\biboptions{numbers,sort\&compress}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{enumitem}
\usepackage{extarrows}

\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{caption}
\usepackage{amsmath}
\usepackage{pifont}
\usepackage{lipsum} % For generating dummy text

\newenvironment{breakablealgorithm}
  {% \begin{breakablealgorithm}
   \begin{center}
     \refstepcounter{algorithm}% New algorithm
     \hrule height.8pt depth0pt \kern2pt% \@fs@pre for \@fs@ruled
     \renewcommand{\caption}[2][\relax]{% Make a new \caption
     {\raggedright\textbf{Algorithm~\thealgorithm} ##2\par}%
     %\textbf{Algorithm~\thealgorithm} ##2\par
       \vskip\belowcaptionskip
       \ifx\relax##1\relax % #1 is \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##2}%
       \else % #1 is not \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
       \fi
       \kern2pt\hrule\kern2pt
     }
  }{% \end{breakablealgorithm}
     \kern2pt\hrule\relax% \@fs@post for \@fs@ruled
   \end{center}
  }
%%%Author macros
\def\tsc#1{\csdef{#1}{\textsc{\lowercase{#1}}\xspace}}
\tsc{WGM}
\tsc{QE}
%%%

\begin{document}
\let\WriteBookmarks\relax
\def\floatpagepagefraction{1}
\def\textpagefraction{.001}
\let\printorcid\relax % 可去掉页面下方的ORCID(s)

% Short title
% \shorttitle{<short title of the paper for running head>} 
\shorttitle{}    

% Short author
% \shortauthors{<short author list for running head>}
\shortauthors{Dengjia Li et al.}

%论文标题
\title[mode = title]{An explainable secure federated collaborative credit modelling method based on blockchain}  

%作者信息
\author[1,3]{Dengjia Li}
\author[1,3]{Chaoqun Ma}
\author[1,3]{Jinglan Yang}
\author[2,3]{Yuncheng Qiao}
\cormark[1]
\ead{qiaoyc@hnu.edu.cn}

\address[1]{Business School, Hunan University, Changsha 410082, China} %声明第一单位
\address[2]{Business School, Shandong University of Technology, Zibo 255000, China} %声明第二单位
\address[3]{Research Institute of Digital Society and Blockchain, Hunan University, China} %声明第三单位
\cortext[1]{Corresponding author}  %声明通讯作者
%\cortext[cor1]{Corresponding author}

% Here goes the abstract
\begin{abstract}
Faced the privacy, security and credible requirements in collaborative credit modelling, and the inherent deficiencies of blockchain and federated learning, this paper proposes a privacy-preserving federated collaborative credit modeling method (BPFL) through federated learning empowered by Hyperledger Fabric blockchain, as well as the privacy guarantee of CKKS homomorphic encryption technology. Specifically, the decentralized storage and learning architecture of Hyperledger Fabric blockchain and federated learning achieves secure and trustworthy circulation of multi-party credit data. CKKS-based BPFL guarantees compliant modeling of vertical federated logistic regression. Moreover, the deployment of SHAP values-based contributivity calculation contracts enhances the explanation of credit default prediction and simultaneously realizes the effective monitoring of all participants. The convergence analysis and experimental system validates the feasibility and effectiveness of the proposed solution.
\end{abstract}

% Use if graphical abstract is present
%\begin{graphicalabstract}
%\includegraphics{}
%\end{graphicalabstract}

% Research highlights
% \begin{highlights}
% \item highlight-1
% \item highlight-2
% \item highlight-3
% \end{highlights}

% Keywords
% Each keyword is seperated by \sep
\begin{keywords}
Credit modelling \sep 
Federated learning \sep 
Blockchain\sep 
CKKS homomorphic encryption\sep 
SHAP values\sep 
\end{keywords}

\maketitle

% Main text
\section{Introduction}
Financial technologies, which are supported and enabled by digital technologies such as big data, the mobile Internet and artificial intelligence, have had a significant impact on the financial system, greatly strengthening the ``public" accessibility of financial services and meeting various ``short, small, frequent, urgent and scattered" credit demands\cite{Giudici:01}. However, with more convenient financial credit activities, the number of credit defaults and instances of fraud have also increased. According to statistics from the central bank, credit cards more than half a year overdue reached 917.5 billion yuan in 2022, and according to the Ministry of Public Security, more than 2 trillion yuan was defrauded in the same year. Therefore, a high-quality credit evaluation model is crucial for credit institutions, banks and customers.

A good credit evaluation model must be supported by multisource and high-quality data. With the rapid development of digital applications such as online shopping and mobile payments, massive and diversified data are being provided to credit institutions, offering crucial decision support for a comprehensive and accurate understanding of the credit status of enterprises and individuals. However, these data are scattered across different organizations and information systems\cite{Yang:11}. The privacy of the data and compliance restrictions on data transmission create considerable barriers to data sharing among multiple institutions\cite{Lan:12}. In addition, abundant false data are mixed in with the massive amounts of data, which has a serious impact on the accuracy of evaluation\cite{Qiao:13}. Therefore, addressing the issues of data authenticity and privacy protection in sharing are crucial for multi-party collaborative modelling.

\par
Unfortunately, existing privacy protection schemes rely mainly on the data sharing-aggregation mode centred on a third party or central server. Since it is difficult to find institutions with sufficient credibility to assume the role of credible third parties, this approach may have considerable security risks\cite{Qiao:14}. Blockchain, utilizing block-chain-based data storage with timestamps, digital signatures, multi-node consensus, and a decentralized transaction processing approach, can ensure the authenticity, immutability, and traceability of credit data, eliminating potential threats to the central server and third party. It provides the possibility of secure data sharing and collaborative modelling of multi-party data\cite{Hassija:15, Yang:16}. However, the public verifiability of the blockchain is a security issue that cannot be ignored, and seriously affects the process of open sharing of data in the blockchain\cite{Qiao:17}. Therefore, how to solve the ``spear" and ``shield" problem between data sharing and privacy protection, public verifiability and privacy protection remains an important problem to solve for blockchain technology.


\par
As a popular distributed machine learning framework, federated learning can realize efficient collaborative modelling between different institutions on the premise that the data of all participants are not domain\cite{Yang:18, Abdulrahman:19, Mothukuri:20, Li:21}. The combination of blockchain and federated learning technology provides a good approach for solving the problems of privacy protection in data sharing and insufficient compliance in data collaborative modelling\cite{Kim:22, Yang:23}. However, federated learning requires shared model parameters (or gradients) in the modelling process, which carries a risk of privacy leakage. Compared to the differential privacy technology, which ensures data privacy at the expense of some data utility, the computability of homomorphic encryption ciphertext realizes the availability of data without affecting the accuracy of the model\cite{Rieyan:24}. Therefore, how to use these technologies to safely and accurately realize multi-party data sharing and collaborative modelling, and achieve a wide range of data and value collaboration has become an important problem to solve in current credit modelling and is also the main goal of our research.


\par
Considering the privacy, security and credible requirements in collaborative credit modelling, by leveraging the decentralized storage and learning architecture of Hyperledger Fabric blockchain and federated learning, combined with CKKS homomorphic encryption technology and SHAP (Shapley Additive Explanations) value contribution calculation contracts, we propose an explainable secure federated collaborative credit modelling method based on blockchain. This approach effectively solves the issues of trustworthiness in collaborative modeling, security and compliance inadequacies in data sharing and aggregation, and the lack of interpretability in assessment results. The contributions of this paper are as follows:

\par
\begin{itemize}
\item Based on the innovative combination of Hyperledger Fabric blockchain and
federated learning, a new architecture (BPFL) is proposed, which is suitable for the secure and trustworthy circulation of multi-party credit data as well as vertical federated logistic regression collaborative modeling. This architecture effectively avoids potential privacy leakage and security threats in traditional centralized model training, while ensuring the consistency of model parameters and the credibility of collaborative modeling.

\item Compared to advanced BFV and Paillier homomorphic encryption, CKKS homomorphic encryption-based BPFL not only effectively achieves secure aggregation of shared gradients in federated learning but also enhances computational efficiency while maintaining model accuracy.

\item The contributivity calculation contract based on Shapley additive explanations (SHAP) clearly demonstrates the feature contribution of each participant, which enhances the explanations of model decisions but also enables effective monitoring by each participant.
\end{itemize}

\par
The remainder of this paper is organized as follows. We begin by introducing some related works in Section 2. Section 3 describes the secure and trustworthy collaborative training architecture. The detailed design of BPFL is given in Section 4. Section 5 demonstrates the convergence of SGD-BPFL and MiniSGD-BPFL. The experiment analysis of BPFL is presented in Section 6. Finally, we concludes this work in Section 7.
%Text of section-1 \cite{Fortunato2010}.

\section{Literature Review}


\subsection{Blockchain-based credit evaluation}
In recent years, financial technology innovation has accelerated the digital and intelligent transformation of finance. Enabled by digital technologies such as big data, artificial intelligence, cloud computing, and blockchain, financial credit institutions are gradually using digital transaction credit as the basis for providing financial services. Ledger data-based credit modeling has become a new paradigm for collaborative credit assessment. Zhang et al.\cite{Zhang:001} mitigated information asymmetry between lenders and borrowers by establishing a credit data sharing consortium.  Chakraborty et al.\cite{Chakraborty:002} improved the credit decision-making process using advanced technologies such as blockchain and machine learning, alleviating the fragmentation of information and enhancing the efficiency of current credit scoring in evaluating creditworthiness. Yang et al.\cite{Yang:16} enhanced the integrity and traceability of credit data based on ledger data storage mechanisms. To prevent potential data leakage risk of the third-party platform and centralized design in the traditional supply chain finance management scheme,  Li et al.\cite{Li:004} achieved secure data storage through the automated execution of smart contracts and attribute-based access control.  Zheng et al.\cite{Zheng:005} combined blockchain and proxy re-encryption technologies to construct a supply chain financial credit system that ensures user security, data security, access security, and sharing security, thereby promoting the secure sharing of corporate credit information and laying the foundation for improving supply chain financing efficiency.

Blockchain, with its traceability, tamper-resistance, and distributed consensus mechanism, provides a trustworthy execution environment for multi-party collaborative credit modeling. However, ensuring data privacy while achieving high data availability is not only a bottleneck of traditional encryption methods but also an inherent problem hindering the open progress of blockchain technology.
\subsection{FL and blockchain-based credit evaluation}
The distributed machine learning architecture of federated learning aligns well with the distributed network structure of blockchain. Without disclosing raw data, participants achieve collaborative credit modeling through model sharing, which enhances the security of the blockchain system to a certain extent\cite{Zhang:006, Banabilah:007, Yang:23}. Additionally, the decentralized execution framework of blockchain effectively mitigates the single point of failure risk associated with the centralized aggregation server in federated learning. Blockchain's immutable, traceable, publicly verifiable, and distributed consensus mechanisms alleviate the trust issues among different participants in federated learning\cite{Zhu:009, Zhang:010}. The dual drive of blockchain and federated learning promotes the secure, trustworthy flow and compliant collaborative modeling of multi-party credit data.

Rückel et al.\cite{Rückel:011} achieved fairness, integrity, and privacy in multivariate linear regression (LR) modeling by innovatively integrating emerging technologies such as blockchain, local differential privacy, and zero-knowledge proofs with the federated learning (FL) architecture, thereby enhancing the practical applicability of FL. Based on on-chain model parameter updates, Zhang et al.\cite{Zhang:012} established a credit-based economic model among participants to ensure fair collaboration in FL. Moreover, the combination of homomorphic encryption and proxy re-encryption technologies forms a key-secure sharing participation permission strategy, ensuring the privacy of participants' identities and information. Addressing security vulnerabilities commonly faced in federated learning, such as model extraction and model inversion attacks, Jia et al.\cite{Jia:013} proposed a secure federated aggregation scheme adapted to Kmeans clustering, random forest and AdaBoost, which combines differential privacy, homomorphic encryption, and blockchain, achieves secure and trustworthy sharing of data and models.

\subsection{Summary}
Despite the significant advantages of federated learning empowered by blockchain and privacy computing in secure, trustworthy data flow and compliant collaborative modeling, current federated learning lacks explainability in its evaluation results, making it difficult to provide reliable credit evaluation services\cite{Jovanovic:014}. It is essential to understand that an explainable federated learning model not only enhances the transparency of credit evaluation services and strengthens users' trust in credit decisions, but also helps identify and understand potential risk factors, thereby further optimizing risk management strategies during decision-making processes. Given the frequent data breaches, credit fraud, and increasing compliance restrictions, the explainability of credit assessment models has attracted considerable attention\cite{Dumitrescu:015,Gunnarsson:016}. However, this area of research is still in its infancy. Although Jovanovic et al.\cite{Jovanovic:014} proposed an automatic credit scoring decision framework with model verifiability, explainable and reliable decision-making by the integration of blockchain and federated learning, it remains a conceptual framework without detailed design or performance evaluation.

\begin{table*}[!hb]
\footnotesize
\setlength{\tabcolsep}{4.8mm}{
\caption{\bf Comparison of our work with other previous works.} \label{tab com}
%\small
\centering
\begin{tabular}{llllll}
     \hline
Paper                     &Blockchain      &Federated    &Privacy          &Explainable               &Detailed\\
                         &                 &learning     &Computation      &Artificial Intelligence    &design\\
\hline
Yang et al.\cite{Yang:16}          & \checkmark      &             &                 &                           &\checkmark\\
Zhang et al.\cite{Zhang:001}         & \checkmark      &             &                 &                           &\checkmark\\
Chakraborty et al.\cite{Chakraborty:002}   & \checkmark      &             &                 &                           &\\
Li et al.\cite{Li:004}            & \checkmark      &             &                 &                           &\checkmark\\
Zheng et al.\cite{Zheng:005}         & \checkmark      &             &                 &                           &\checkmark\\
 \hline
Rückel et al.\cite{Rückel:011}        & \checkmark      &\checkmark   &\checkmark       &                           &\checkmark\\
Zhang et al.\cite{Zhang:012}         & \checkmark      &\checkmark   &\checkmark       &                           &\checkmark\\
Jia et al.\cite{Jia:013}           & \checkmark      &\checkmark   &\checkmark       &                           &\checkmark\\
 \hline
Jovanovic et al.\cite{Jovanovic:014}     & \checkmark      &\checkmark   &\checkmark       &\checkmark                 &\\
Dumitrescu et al.\cite{Dumitrescu:015}    &                 &             &                 &\checkmark                 &\checkmark\\
Gunnarsson et al.\cite{Gunnarsson:016}    &                 &             &                 &\checkmark                 &\checkmark\\
 \hline
 Our work                &\checkmark       &\checkmark   &\checkmark       &\checkmark                 &\checkmark\\
 \hline
\end{tabular}}
\end{table*}

Table \ref{tab com} summarizes previous efforts to develop secure, trustworthy, and privacy-preserving credit assessment systems, showing the gap in the literature addressed in the current study. For few works, trustworthy, privacy, and explainability tradeoff are null. BPFL achieves trustworthy data flow, compliant collaborative modeling compared to existing works and guarantees explainability with detailed design and performance evaluation.

\section{Preliminaries}
\subsection{CKKS Homomorphic encryption}
Unlike the BFV scheme\cite{Brakerski:01}, which only supports integer addition and multiplication homomorphic operations, and the Paillier scheme\cite{Paillier:01}, which only supports addition homomorphism, CKKS (Cheon-Kim-Kim-Song)\cite{Cheon:01} is a scheme that supports homomorphic operations on encrypted real and complex numbers. This makes it very useful in applications that need to handle floating-point numbers. Through approximate computation, CKKS can significantly reduce computational complexity and improve the efficiency of homomorphic operations. The CKKS scheme consists of three basic algorithms: Key Generation (KeyGen), Encryption (Enc), and Decryption (Dec)\cite{Lin:01}.
\par
\begin{itemize}
\item \textbf{Key Generation:} $KeyGen(1^{\lambda})\rightarrow(pk,sk,evk)$\\
where, $\lambda$ is the security parameter, $q_i$ is the ciphertext modulus, and $q_i=p^{i}\cdot q_0, i=0,1,\cdots,L$, $p$ acts as a foundational base, scaling each operation, $sk=s\in \mathcal{R}_q, pk=(\natural,\sharp), \natural\in \mathcal{R}_q, \sharp=-\natural\cdot s+e$, $\mathcal{R}_q$ refers to the polynomial ring modulo $q$,  $\mathcal{R}_q=\mathbb{Z}_q[x]/<f(x)>$, $e$ is an error polynomial sampled from the error distribution $\chi$.
\end{itemize}

\begin{itemize}
\item \textbf{Encryption:} $Enc(pk,m)\rightarrow(c_0,c_1)$\\
where, $c_0=p\cdot \bigtriangleup\cdot m+\natural\cdot r+e_1$, $c_1=\sharp\cdot r+e_2$, $\bigtriangleup$ is a pre-selected constant used to scale real or complex messages into integer form, $e_1$ and $e_1$ are error polynomials.
\end{itemize}

\begin{itemize}
\item \textbf{Decryption:} $Dec(s,c)\rightarrow(m^{\prime})$\\
where, $c=(c_0,c_1)$, $m^{\prime}=\frac{1}{\Delta}\cdot(Round(c_0+c_1\cdot s))$, $Round(\cdot)$ is used to round the result to the nearest integer, reducing errors caused by floating-point representation.
\end{itemize}

\subsection{CKKS Homomorphic Operations}
\begin{itemize}
\item \textbf{Homomorphic Addition:} $c_{Add}=(c_{0,1}+c_{0,2},c_{1,1}+c_{1,2})$
\end{itemize}

\begin{itemize}
\item \textbf{Homomorphic Multiplication:} $c_{mult}=(Rescale(c_{0,1}\cdot c_{0,2}), Rescale(c_{1,1}\cdot c_{1,2})$\\
where, $Rescale(\cdot)$  is used to reduce the modulus of the ciphertext after homomorphic multiplication, while also reducing the scale factor of the ciphertext. $Rescale(c)=([\frac{c_0}{p}],[\frac{c_1}{p}])$, which helps control the growth of ciphertext noise and maintains an appropriate ciphertext size.
\end{itemize}

\begin{itemize}
\item \textbf{Rotation:} $c_{rot}=(Rotate(c_{0},k), Rotate(c_{1},k))$\\
where, the rotation operation $Rotate(\mathbf{c},k)$ cyclically shifts the elements of the vector by $k$ positions. Here, $k$ can be positive (shift to the right) or negative (shift to the left), i.e., $Rotate(\mathbf{c},k)=(c_{n-k},c_{n-k+1},\cdots,c_{n-1},c_{0},$\\
$c_{1},\cdots,c_{n-k-1}), \mathbf{c}=(c_{0},c_{1},\cdots,c_{n-1})$.
\end{itemize}

\section{System architecture}
\subsection{Architecture of BPFL}
To address the issues of traditional coordinator-centered federated collaborative training security and compliance deficiencies, as shown in Figure \ref{cta}, this paper provides a secure and trustworthy collaborative training architecture (BPFL) for interpretable logistic model utilizing decentralized execution architecture, immutability, and chaincode automatic execution of Hyperledger Fabric blockchain. BPFL is composed of three components: Peer-to-Peer (P2P) network layer, Ledger layer, and Application layer.
 \begin{figure*}[!hb]
\centering
\includegraphics[width=10.35cm, height=6.25cm]{Figure//CTA.png}
\caption{A secure and trustworthy collaborative training architecture.}
\label{cta}
\end{figure*}

\subsubsection{P2P network layer}
P2P network is a decentralized network architecture that operates without a central authority or server, making it more resilient to failures and attacks. Each node is autonomous and can independently initiate or complete tasks, rendering the network dynamic and self-organizing. Nodes in different geographical regions can directly share information with each other, enabling credit institutions to quickly perform cross-organizational collaborative modeling. The network layer provides essential functional modules for the secure and compliant operation of a vertical federated logistic regression model. These modules include Fabric CA, which is responsible for issuing, managing, and revoking certificates for entities within the network, endorsement policies, ordering services, distributed consensus, and chaincode (also known as smart contracts) related to the operation of the blockchain network.

In the network, multiple computing nodes are randomly selected to replace the centralized coordinator, combined with CKKS homomorphic encryption technology, to achieve the compliance, efficient interaction and aggregation of gradient information between party A and party B. The privacy-preserving Taylor loss function and the corresponding gradients for interpretability logistic model are as follow.
\begin{eqnarray}\label{HL1}
\nonumber
[[\mathfrak{L}]]&=&\frac{1}{n}\sum\limits_{i=1}^{N}\{ -y_i([[\varpi_i^{A}]]+[[\varpi_i^{B}]])+\frac{1}{2}([[\varpi_i^{A}]]+[[\varpi_i^{B}]])\\
&+&\frac{1}{8}[[(\varpi_i^{A})^2]]+\frac{1}{8}[[(\varpi_i^{B})^2]]
+\frac{1}{4}[[\varpi_i^{A}\times \varpi_i^{B}]]\},
\end{eqnarray}



\begin{eqnarray}\label{GL2}
\nonumber
[[\frac{\partial\mathfrak{L}}{\partial \theta_A}]]&=&
\frac{1}{n}\{\sum\limits_{i=1}^{N}-[[y_ix_i^{A}]]
+\frac{1}{2}[[x_i^{A}]]+\frac{1}{4}[[\varpi_i^{A}x_i^{A}]]\\
&+&\frac{1}{4}[[\varpi_i^{B}]]x_i^{A}\},
\end{eqnarray}

\begin{eqnarray}\label{GL3}
\nonumber
[[\frac{\partial\mathfrak{L}}{\partial \theta_B}]]&=&\frac{1}{n}\{\sum\limits_{i=1}^{N}-[[y_ix_i^{B}]]
+\frac{1}{2}[[x_i^{B}]]+\frac{1}{4}[[\varpi_i^{B}x_i^{B}]]\\
&+&\frac{1}{4}[[\varpi_i^{A}]]x_i^{B}\},
\end{eqnarray}
where, $\theta$ are coefficients of features to be learned by the model, $\theta=[\theta^{A}, \theta^{B}]$, $\varpi_i^{A}=\theta^{A}x_i^{A}, \varpi_i^{B}=\theta^{B}x_i^{B},$ $[[\cdot]]$ is CKKS homomorphic encryption operator. $[[\cdot]]_A$, $[[\cdot]]_B$, and $[[\cdot]]_C$ are CKKS homomorphic encryption operators with public keys from party A, B, and C.

If most of the computing nodes (generally more than two-thirds) can obtain the same calculation result, then the aggregation results reach consensus, and the aggregation results are subsequently uploaded to the blockchain.


\subsubsection{Ledger layer}
The ledger layer in Hyperledger Fabric is the core component responsible for storing blockchain data, ensuring data integrity and immutability. The ledger layer primarily consists of the Blockchain Log, which records data in a sequential block-chain, and the World State, which is managed by the LevelDB storage engine for key-value storage, querying, and updating (though CouchDB, a document-oriented NoSQL database suitable for storing and querying complex JSON documents, is also commonly used as a storage engine; this article focuses on the LevelDB storage engine as an example).

In the blockchain, only the hash values related to shared data during federated collaborative credit modelling (e.g., R:SHA256) and identifying information such as the hash values of the original data (e.g., SHA256(CD), SHA256(DD), SHA256(PD), SHA256(AD)) are stored. The Blockchain Log structure of the blockchain-based federated collaborative credit evaluation system mainly comprises two parts: block header and block body. As shown in Figure \ref{ONC}, the block header records the identifying information of the current block and its contained transactions, while the block data and block metadata together form the block body to store all transactions and their related verification information.

 \begin{figure*}[!ht]
\centering
\includegraphics[width=12.35cm, height=5.75cm]{Figure//ONC.png}
\caption{Blockchain Structure.}
\label{ONC}
\end{figure*}


In the block header, BlockNum is the sequence number of the block, Block Hash is the hash value of the current block, PreHash is the hash value of the previous block, Timestamp is the timestamp of block creation, ChanNum is the channel number to which the current block belongs, Data Hash represents the hash value of all transaction data in the current block, and MerkleRoot is the Merkle tree root of all transaction data in the block, enabling fast transaction verification. The block body contains the transactions and their related validation information, as well as the creator's validation information. Within this, TransID represents the transaction ID, ValiCode is the transaction validation code, CreMSP is the MSP name of the transaction creator, Endorser is the endorser of the transaction, TransTime is the creation time of the transaction, and TransSig is the transaction signature.

The World State is a representation of the current state of the blockchain, storing the latest key-value pair data. It provides a method for quickly accessing the current valid data without needing to traverse the entire Blockchain Log. As shown in Figure \ref{SA}, by setting parameters such as block number, block hash, and transaction ID as keys, and setting the block path as the value in a key-value storage, a corresponding index list for the blocks is generated and stored in LevelDB. Using these key-value indexes, data requesters can quickly query the off-chain stored information corresponding to them. Simultaneously, by verifying the hash value of the off-chain data with the hash value of the corresponding on-chain data, the validity and integrity of the off-chain data can be ensured.
 \begin{figure*}[!ht]
\centering
\includegraphics[width=12.35cm, height=6.25cm]{Figure//SA.png}
\caption{Blockchain encrypted storage structure.}
\label{SA}
\end{figure*}

\subsubsection{Application layer}
The physical application layer of the blockchain is an application client. In this layer, participants can use mobile phones, computers, and other terminal devices to interact with the relevant chaincode via the Fabric Software Development Kit (SDK), facilitating interactions and collaborative modeling with other participants without needing to understand the underlying blockchain technology. Application Programming Interfaces (APIs) serve as a good practice to abstract this process, making it easier to connect applications and various participating entities.

%\subsection{Threat model and design goal}

\section{Detailed design of the BPFL system}

\subsection{Blockchain-based private set intersection(BC-PSI)}
Considering credit datasets with vertical distribution characteristics, i.e., datasets with the same sample space but different feature spaces, and to avoid the risk of privacy leakage brought by traditional centralized ID comparison, a smart contract-based decentralized private set intersection (BC-PSI) method is proposed. This method completes the common sample ID comparison of all participants by uploading encrypted ID information on the blockchain. Assuming $N$ participants, the sample $ID$ set for each participant is$\{U_1,U_2,U_3,\cdots,U_N\}$. The detailed BC-PSI method as follows.

\subsubsection{System initialization}
Each participant $P_i$ generates its own public key $pk_i$ and private key pair $sk_i$ on the blockchain. Specifically, Fabric-CA generates the global initialization system parameter $(\mathbb{G},G)$ under given security parameters $\lambda$. Each participant $P_i$ selects a random exponent $\alpha\in \mathbb{G}_{p}^{*}$ and uses this random exponent $\alpha$ as the private key $sk_{i}$. Using the generator $G$, they generate the public key $pk_{P_i}$, i.e., $\mathcal{G}(\alpha, G)\rightarrow pk_{i}$, where $sk_{DP}=\alpha, pk_{DP}=\alpha G.$ Each participant then broadcasts their public key to the blockchain, and upon reaching consensus, the $Block_{t}$ is updated.  Algorithm 1 outlines the whole process.

\begin{breakablealgorithm}
\caption{Initialization}
\small
\hspace*{0.02in}\raggedright {\bf Input:}
Participants~$P=\{P_i\}_{i=1}^{N}$\\
\begin{algorithmic}[1]
\State
$(\mathbb{G},G)\longleftarrow Setup(1^{\lambda})$
\For{Each participant $P_i$}
\State
$(pk_{i}, sk_{i}) \longleftarrow \mathcal{G}(\alpha,G)$
\EndFor
\State
$ Block_{t}\leftarrow\{pk_i\}$
\State
{\bf broadcast}~$Block_{t}$~to all participants
\For{Each participant~$i,~i=1, 2, \cdots, n$~{\bf in parallel}}
\State
{\bf update}~$Block_{t}$~to the global ledger.
\EndFor
\end{algorithmic}
\end{breakablealgorithm}
\subsubsection{Encrypted data uploading and storage}

To protect the privacy of participant identities and sample $ID$, each participant $i$ hashes their own sample $ID$, $SHA_{U_i}$, and then encrypts it to obtain the encrypted dataset $Y_i$ using their own public key. Uploading the encrypted dataset $Y_i$ to blockchain, and updating~$\{Y_i, pk_i\}$~to the global ledger. $Y_i$ is associated with each participant's identity and public key  through the use of smart contracts. Algorithm 2 delineates the detailed encrypted data upload and storage(EUS) procedure.

\begin{breakablealgorithm}
\caption{EUS Algorithm}
\small
\hspace*{0.02in}\raggedright {\bf Input:}
Sample~$ID=\{U_i\}_{i=1}^{N}$\\
%\hspace*{0.02in} \raggedright{\bf Output:}
%Encrypted Data $Y_i$
\begin{algorithmic}[1]
\For{Each sample~$ID$~in $\{U_i\}$}
    \State $SHA_{U_i}=Hash(U_i)$
     \State $Y_i= Encrypt(SHA_{U_i}, pk_i)$
\EndFor
\State
$ Block_{t+1}\leftarrow\{Y_i, pk_i\}$
\State
{\bf broadcast}~$Block_{t+1}$~to all participants
\For{Each participant~$i,~i=1, 2, \cdots, n$~{\bf in parallel}}
\State
{\bf update}~$Block_{t+1}$~to the global ledger.
\EndFor
\end{algorithmic}
\end{breakablealgorithm}


\subsubsection{Private set intersection}
Each participant $i$ retrieves the encrypted dataset $Y_j (i\neq j)$ from blockchain and encrypts its own dataset using the public key of the other $N-1$ participant. The encrypted $N-1$ dataset is compared with the encrypted dataset of other nodes to find the common data sample $U_{common_k}$ of participant $i$. The results are uploaded to the blockchain, and the smart contract is invoked to determine the minimum intersection $\{U_{common_1},U_{common_2},\cdots, U_{common_k}\}$. The Private Set Intersection(PSI) algorithm is implemented in Algorithm 3.


\begin{breakablealgorithm}
\caption{PSI Algorithm}
\small
\hspace*{0.02in}\raggedright {\bf Input:}
Participants~$P=\{P_i\}_{i=1}^{N}$, and Sample~$ID=\{U_i\}_{i=1}^{N}$\\
\begin{algorithmic}[1]
\For{Each sample~$ID$~in $\{U_i\}$}
    \If{$j!= i$}
\State $Y_j=Download(SHA_{U_j})$
\State $Y_{i,j}=Encrypt(SHA_{U_i}, pk_j)$
\EndIf
\State $U_{common_k}=common_{ID}.intersection\{Y_{i,j},Y_{j}\}$
\EndFor
\State
$Block_{t+k}\leftarrow\{U_{common_1},U_{common_2},\cdots, U_{common_k},pk_i\}$
\State
{\bf broadcast}~$Block_{t+k}$~to all participants
\For{Each participant~$i,~i=1, 2, \cdots, n$~{\bf in parallel}}
\State
{\bf update}~$Block_{t+k}$~to the global ledger.
\EndFor
\end{algorithmic}
\end{breakablealgorithm}

\subsection{Blockchain-based privacy-federated collaborative logistic regression modelling(BPFL)}

The entire process of BPFL is summarized into four main stages that interact
involve interactions among different entities, namely, data encryption and sharing(DES), secure collaborative modeling(SCM), model aggregation and decryption(MAD), and federated model training(FMT). Taking the logistic regression modelling by participants A and B as an example, the complete workflow of BPFL as follows.

\subsubsection{Data encryption and sharing(DES)}
\par
\ding{172}~Party A calculates $\varpi_i^{A}$ and $(\varpi_i^{A})^2$, and then encrypts them as $[[\varpi_i^{A}]]_A$ and $[[(\varpi_i^{A})^2]]_A$ by CKKS homomorphic encryption, respectively. Uploading these encrypted coefficients to blockchain.

\par
\ding{173}~Similarity, party B calculates $\varpi_i^{B}$ and $(\varpi_i^{B})^2$ by CKKS homomorphic encryption, and then uploads its encrypted $[[\varpi_i^{B}]]_B$, $[[(\varpi_i^{B})^2]]_B$ to blockchain.

\begin{breakablealgorithm}
\caption{DES Algorithm}
\small
\hspace*{0.02in}\raggedright {\bf Input:}
$\theta^{\pi}$, $\mathfrak{D}=(x_i^{\pi}, y_i^{\pi})$, $\pi=A,B$\\
%\hspace*{0.02in} \raggedright{\bf Output:}
%$\theta$
\begin{algorithmic}[1]
\For{Party~$\pi$, $\pi=A, B$~{\bf in parallel}}
\State
$(\varpi_i^{\pi}, (\varpi_i^{\pi})^2) \longleftarrow (\theta^{\pi},x_i^{\pi})$.
\State
$([[\varpi_i^{\pi}]]_\pi, [[(\varpi_i^{\pi})^2]]_\pi) \longleftarrow (\varpi_i^{\pi}, (\varpi_i^{\pi})^2).$
\EndFor
\State
$Block_{t+l}\leftarrow\{[[\varpi_i^{\pi}]]_\pi, [[(\varpi_i^{\pi})^2]]_\pi, pk_\pi\}$
\State
{\bf broadcast}~$Block_{t+l}$~to all participants
\For{Each participant~$i,~i=1, 2, \cdots, n$~{\bf in parallel}}
\State
{\bf update}~$Block_{t+l}$~to the global ledger.
\EndFor
\end{algorithmic}
\end{breakablealgorithm}


\subsubsection{Secure collaborative modelling(SCM)}
\par
\ding{174}~Downloaded $[[\varpi_i^{B}]]_B$ and $[[(\varpi_i^{B})^2]]_B$ from blockchain, party A computes $[[\mathfrak{L}]]_B$ and $[[\frac{\partial\mathfrak{L}}{\partial \theta_A}]]_B$ via (\ref{HL1}) and (\ref{GL2}). Then party A creates random mask $\tau_A$ and adds it to $[[\frac{\partial\mathfrak{L}}{\partial \theta_A}]]_B$ to obtain $[[\frac{\partial\mathfrak{L}}{\partial \theta_A}+\tau_A]]_B$. Uploading $[[\mathfrak{L}]]_B$ and $[[\frac{\partial\mathfrak{L}}{\partial \theta_A}+\tau_A]]_B$ to blockchain.

\par
\ding{175}~Downloaded $[[\mathfrak{L}]]_B$, $[[\frac{\partial\mathfrak{L}}{\partial \theta_A}+\tau_A]]_B$, and $[[\varpi_i^{A}]]_A$ from blockchain, party B decrypts $[[\mathfrak{L}]]_B$, $[[\frac{\partial\mathfrak{L}}{\partial \theta_A}+\tau_A]]_B$, and computes $[[\frac{\partial\mathfrak{L}}{\partial \theta_B}]]_A$ via (\ref{GL3}). Then party B creates random mask $\tau_B$ and adds it to $[[\frac{\partial\mathfrak{L}}{\partial \theta_B}]]_A$ to obtain $[[\frac{\partial\mathfrak{L}}{\partial \theta_B}+\tau_B]]_A$. Uploading $[[\frac{\partial\mathfrak{L}}{\partial \theta_B}+\tau_B]]_A$ and $[[\frac{\partial\mathfrak{L}}{\partial \theta_A}+\tau_A]]_C$ to blockchain.

\par
\ding{176}~Party A decrypts $[[\frac{\partial\mathfrak{L}}{\partial \theta_B}+\tau_B]]_A$ downloaded from blockchain, and uploads the encrypted $[[\frac{\partial\mathfrak{L}}{\partial \theta_B}+\tau_B]]_C$ to blockchain.

\begin{breakablealgorithm}
\caption{SCM Algorithm}
\small
\hspace*{0.02in}\raggedright {\bf Input:}
$\theta^{\pi}$, $\mathfrak{D}=(x_i^{\pi}, y_i^{\pi})$, $\pi=A,B$\\
%\hspace*{0.02in} \raggedright{\bf Output:}
%$\theta$
\begin{algorithmic}[1]
\For{Party $A$}
\State
$([[\varpi_i^{B}]]_B, [[(\varpi_i^{B})^2]]_B)\longleftarrow Download(Block_{t+l},pk_B)$.
\State
$[[\mathfrak{L}]]_B\longleftarrow ([[\varpi_i^{A}]]_A, [[(\varpi_i^{A})^2]]_A, [[\varpi_i^{B}]]_B, [[(\varpi_i^{B})^2]]_B)$.
\State
$[[\frac{\partial\mathfrak{L}}{\partial \theta_A}]]_B
\longleftarrow([[y_ix_i^{A}]],[[x_i^{A}]],[[\varpi_i^{A}x_i^{A}]],
[[\varpi_i^{B}]]x_i^{A})$.
\State
$[[\frac{\partial\mathfrak{L}}{\partial \theta_A}+\tau_A]]_B
\longleftarrow (\frac{\partial\mathfrak{L}}{\partial \theta_A},\tau_A)$.
\EndFor
\State
$Block_{t+m}\leftarrow\{[[\mathfrak{L}]]_B, [[\frac{\partial\mathfrak{L}}{\partial \theta_A}+\tau_A]]_B\}$
\State
{\bf broadcast}~$Block_{t+m}$~to all participants
\For{Each participant~$i,~i=1, 2, \cdots, n$~{\bf in parallel}}
\State
{\bf update}~$Block_{t+m}$~to the global ledger.
\EndFor
\For{Party $B$}
\State
$([[\mathfrak{L}]]_B, [[\frac{\partial\mathfrak{L}}{\partial \theta_A}+\tau_A]]_B)\longleftarrow Download(Block_{t+m},pk_A)$.
\State
$[[\frac{\partial\mathfrak{L}}{\partial \theta_B}]]_A
\longleftarrow([[y_ix_i^{B}]],[[x_i^{B}]],[[\varpi_i^{B}x_i^{B}]],
[[\varpi_i^{A}]]x_i^{B})$.
\State
$[[\frac{\partial\mathfrak{L}}{\partial \theta_B}+\tau_B]]_A
\longleftarrow (\frac{\partial\mathfrak{L}}{\partial \theta_B},\tau_B)$.
\State
$[[\frac{\partial\mathfrak{L}}{\partial \theta_A}+\tau_A]]
\longleftarrow Dec([[\frac{\partial\mathfrak{L}}{\partial \theta_A}+\tau_A]]_B,sk_B)$.
\State
$[[\frac{\partial\mathfrak{L}}{\partial \theta_A}+\tau_A]]_C
\longleftarrow Enc(\frac{\partial\mathfrak{L}}{\partial \theta_A}+\tau_A,pk_C)$.
\EndFor
\State
$Block_{t+n}\leftarrow\{[[\frac{\partial\mathfrak{L}}{\partial \theta_B}+\tau_B]]_A,
[[\frac{\partial\mathfrak{L}}{\partial \theta_A}+\tau_A]]_C\}$
\State
{\bf broadcast}~$Block_{t+n}$~to all participants
\For{Each participant~$i,~i=1, 2, \cdots, n$~{\bf in parallel}}
\State
{\bf update}~$Block_{t+n}$~to the global ledger.
\EndFor
\For{Party $A$}
\State
$([[\frac{\partial\mathfrak{L}}{\partial \theta_B}+\tau_B]]_A)\longleftarrow Download(Block_{t+l},pk_C)$.
\State
$[[\frac{\partial\mathfrak{L}}{\partial \theta_B}+\tau_B]]
\longleftarrow Dec([[\frac{\partial\mathfrak{L}}{\partial \theta_B}+\tau_B]]_A,sk_A)$.
\State
$[[\frac{\partial\mathfrak{L}}{\partial \theta_B}+\tau_B]]_C
\longleftarrow Enc(\frac{\partial\mathfrak{L}}{\partial \theta_B}+\tau_B,pk_C)$.
\EndFor
\State
$Block_{t+r}\leftarrow\{[[\frac{\partial\mathfrak{L}}{\partial \theta_B}+\tau_B]]_C\}$
\State
{\bf broadcast}~$Block_{t+r}$~to all participants
\For{Each participant~$i,~i=1, 2, \cdots, n$~{\bf in parallel}}
\State
{\bf update}~$Block_{t+r}$~to the global ledger.
\EndFor
\end{algorithmic}
\end{breakablealgorithm}

\subsubsection{Model aggregation and decryption(MAD)}
\par
\ding{177}~Computing node C downloaded $[[\frac{\partial\mathfrak{L}}{\partial \theta_A}+\tau_A]]_C$, $[[\frac{\partial\mathfrak{L}}{\partial \theta_B}+\tau_B]]_C$ from blockchain, and then decrypts $[[\frac{\partial\mathfrak{L}}{\partial \theta_A}+\tau_A]]_C$, $[[\frac{\partial\mathfrak{L}}{\partial \theta_B}+\tau_B]]_C$ as $\frac{\partial\mathfrak{L}}{\partial \theta_A}+\tau_A$, $\frac{\partial\mathfrak{L}}{\partial \theta_B}+\tau_B$, respectively. Uploading them to blockchain.

\begin{breakablealgorithm}
\caption{MAD Algorithm}
\small
\hspace*{0.02in}\raggedright {\bf Input:}
$\theta^{\pi}$, $\mathfrak{D}=(x_i^{\pi}, y_i^{\pi})$, $\pi=A,B$\\
\begin{algorithmic}[1]
\For{Computing node $C$}
\State
$([[\frac{\partial\mathfrak{L}}{\partial \theta_{\pi}}+\tau_{\pi}]]_C,)\longleftarrow Download(Block_{t+n},Block_{t+r})$.
\State
$\frac{\partial\mathfrak{L}}{\partial \theta_A}+\tau_A
\longleftarrow Dec([[\frac{\partial\mathfrak{L}}{\partial \theta_A}+\tau_A]]_C,sk_C)$.
\State
$\frac{\partial\mathfrak{L}}{\partial \theta_B}+\tau_B
\longleftarrow Dec([[\frac{\partial\mathfrak{L}}{\partial \theta_B}+\tau_B]]_C,sk_C)$.
\EndFor
\State
$Block_{t+s}\leftarrow\{\frac{\partial\mathfrak{L}}{\partial \theta_A}+\tau_A,\frac{\partial\mathfrak{L}}{\partial \theta_B}+\tau_B\}$
\State
{\bf broadcast}~$Block_{t+s}$~to all participants
\For{Each participant~$i,~i=1, 2, \cdots, n$~{\bf in parallel}}
\State
{\bf update}~$Block_{t+s}$~to the global ledger.
\EndFor
\end{algorithmic}
\end{breakablealgorithm}
\subsubsection{Federated model training(FMT)}
\par
\ding{178}~Party A and B download $\frac{\partial\mathfrak{L}}{\partial \theta_A}+\tau_A$, $\frac{\partial\mathfrak{L}}{\partial \theta_B}+\tau_B$, respectively. Then, party A obtains gradient $\frac{\partial\mathfrak{L}}{\partial \theta_A}$ by subtracting $\tau_A$; party B obtains gradient $\frac{\partial\mathfrak{L}}{\partial \theta_B}$ by subtracting $\tau_B$.


\par
\ding{179}~Using Mini-batch Stochastic Gradient Descent (abbreviated as MiniSGD), Party A and B update their respective model parameters according to \ref{th1} and \ref{th2} once the loss $\mathfrak{L}$ converges. Otherwise, goes to step 1 to continue the collaborative training process.
\begin{eqnarray}\label{th1}
\theta^{A}\doteq \theta^{A}-\eta\frac{\partial\mathfrak{L}}{\partial \theta_A},
\end{eqnarray}
\begin{eqnarray}\label{th2}
\theta^{B}\doteq \theta^{B}-\eta\frac{\partial\mathfrak{L}}{\partial \theta_B},
\end{eqnarray}
where, $\eta$ is learning rate.

\begin{breakablealgorithm}
\caption{FMT Algorithm}
\small
\hspace*{0.02in}\raggedright {\bf Input:}
$\theta^{\pi}$, $\mathfrak{D}=(x_i^{\pi}, y_i^{\pi})$, $\pi=A,B$\\
\begin{algorithmic}[1]
\For{Party~$\pi$, $\pi=A, B$~{\bf in parallel}}
\State
$\frac{\partial\mathfrak{L}}{\partial \theta_\pi}+\tau_\pi \longleftarrow Download(Block_{t+s})$.
\State
$\frac{\partial\mathfrak{L}}{\partial \theta_\pi}=(\frac{\partial\mathfrak{L}}{\partial \theta_\pi}+\tau_\pi)-\tau_\pi$.
\State
allocate $\mathfrak{D}_i \sim \mathfrak{D}$ to $\pi$
\State
$\mathfrak{B}\longleftarrow$ split $\mathfrak{D}_i$ into batches of size $\mathfrak{B}$
\For{each epoch $i$ from 1 to $\mathfrak{E}$}
\For{each epoch $b\in\mathfrak{E}$}
\State
$\theta^{\pi}\longleftarrow \theta^{\pi}-
\frac{1}{|\mathfrak{B}|}\eta\frac{\partial\mathfrak{L}}{\partial \theta_\pi}.$
\EndFor
\EndFor
\EndFor
\State
$Block_{t+\nu}\leftarrow\{\partial\mathfrak{L},\theta^{\pi}\}$
\State
{\bf broadcast}~$Block_{t+\nu}$~to all participants
\For{Each participant~$i,~i=1, 2, \cdots, n$~{\bf in parallel}}
\State
{\bf update}~$Block_{t+\nu}$~to the global ledger.
\EndFor
\end{algorithmic}
\end{breakablealgorithm}
%\subsection{Computing explanations based on SHAP values}

\subsection{SHAP values-based contributivity calculation(SHAP-C)}
To enhance the explanations of credit default prediction, the SHAP values-based contributivity calculation contract(SHAP-C) is deployed. Specifically, it calculates the contribution of each feature subset to the model prediction output, and uses these contributions to calculate the SHAP values of each feature to quantify the contribution of each feature to the final output.

The calculation formula of SHAP is as follows:
\begin{eqnarray}\label{sh1}
\nu_i^{k}=\sum\limits_{T\subseteq I} (\phi_i^{k}(T)),
\end{eqnarray}
where $\nu_i^{k}$ represents the output of the feature set $I$ at the $k$th sample point. Specifically, the SHAP value $\phi_i^{k}(T)$ of the $i$th feature $x_i$ at the $k$th sample point can be expressed as follows:

\begin{eqnarray}\label{sh2}
\phi_i^{k}(T)=\frac{|T|!(|I|-|T|-1)!}{|I|!}[\nu_{T\bigcup \{i\}}^{k}-\nu_{T}^{k}],
\end{eqnarray}
where $\phi_i^{k}(T)$ represents the contribution of the $i$th feature to the output when considering subset $T$, $T$ is a subset of the feature set $I$, $|T|$ represents the number of elements in set $T$,  $\nu_{T\bigcup \{i\}}^{k}$ represents the model output including feature $i$, and $\nu_{T}^{k}$ represents the model output without including feature $i$. Algorithm 8 delineates the detailed explainable credit default prediction procedure.

\begin{breakablealgorithm}
\caption{SHAP-C Algorithm}
\small
\hspace*{0.02in}\raggedright {\bf Input:}
model, X, feature set I, k-th sample point \\
\begin{algorithmic}[1]
\For{each feature $i$ in $I$}
    \State $\phi_i^{k} \leftarrow 0$
    \For{each subset $T \subseteq I \setminus \{i\}$}
        \State $T_{\bigcup \{i\}} \leftarrow T \cup \{i\}$
        \State $\nu_T^{k} \leftarrow model.predict(X_T)$
        \State $\nu_{T_{\bigcup \{i\}}}^{k} \leftarrow model.predict(X_{T_{\bigcup \{i\}}})$
        \State $weight \leftarrow \frac{|T|!(|I|-|T|-1)!}{|I|!}$
        \State $\phi_i^{k} \leftarrow \phi_i^{k} + weight \times (\nu_{T_{\bigcup \{i\}}}^{k} - \nu_T^{k})$
    \EndFor
    \State $\nu_i^{k} \leftarrow \sum\limits_{T\subseteq I} (\phi_i^{k}(T))$
\EndFor
\end{algorithmic}
\end{breakablealgorithm}

\section{Convergence analysis of BPFL}
Compared to using the entire dataset, MiniSGD in Section 5.2.4 allows for rapid iteration and accelerated computation, helping the model converge more smoothly. Under the assumptions of convex functions and bounded gradient variance, this section provides a detailed proof of the convergence of SGD-BPFL and MiniSGD-BPFL.

\subsection{Convergence of SGD-BPFL}
Assumption 1 (Lipschitz continuity). The global loss function $f(\cdot)$ is L-Lipschitz continuous, i.e., for any parameter vector $\theta_1, \theta_2$, there is

\begin{eqnarray}\label{ap1}
\|f(\theta_1)-f(\theta_2)\|\leq L\|\theta_1-\theta_2\|.
\end{eqnarray}

Assumption 2 (Bounded gradient).  The gradient of the local loss function in each client is bounded, i.e., for an arbitrary client $k$ and the parameter vector $\theta_1, \theta_2$, there is

\begin{eqnarray}\label{ap2}
\|\nabla_k f(\theta)\|\leq G.
\end{eqnarray}

Assumption 3 (Bounded variance). The variance of stochastic gradients in each client is bounded, i.e., for any client $k$ and the parameter vector $\theta$, there is
\begin{eqnarray}\label{ap1}
E[\|\nabla_k f(\theta)-\nabla f(\theta)\|^2]\leq \sigma^2.
\end{eqnarray}

Theorem 1. Let $f(\cdot)$ be a convex function that satisfies Assumptions 1 to 3, $\theta^{*}\in argmin_{\theta:\parallel\theta\parallel\leq B}f(\theta)$. If the SGD-BPFL runs for $T$ iterations with a step size $\eta=\sqrt{\frac{G^2}{\sigma^2T}}$, then the following holds,
\begin{eqnarray}
E[f(\theta)] - f(\theta^{*})\leq \frac{G\sigma}{T}.
\end{eqnarray}

Moreover, for any $\epsilon>0$, to satisfy $E[f(\bar{\theta}))] - f(\theta^{*})\leq \epsilon$, the number of iterations required by the SGD-BPFL must satisfy
$T\geq\frac{G^2\sigma^2}{\epsilon^2}$.

%The detailed proof of Theorem 1 as follows, which is further supported by the experimental results in Section 6.

{\bf Proof.} Assume
\begin{eqnarray*}
&&E[f(\overline{\theta_A+\theta_B})]-f(\theta^{*})\\
&=&E[f(\overline{\theta_A+\theta_B})-f(\theta^{*})]\\
&=&E[\frac{1}{T}\sum\limits_{T+1}^{t=1}(f(\theta_A+\theta_B)^{(t+1)})-f(\theta^{*})]\\
&\leq&E[\frac{1}{T}\sum\limits_{T+1}^{t=1}<(\theta_A+\theta_B)^{(t+1)}-\theta^{*},\nabla f(\theta_A+\theta_B)>].
\end{eqnarray*}

Expanding the inner product,

\begin{eqnarray*}
&&\sum\limits_{t=1}^{T+1}<(\theta_A+\theta_B)^{(t)}-\theta^{*},\nabla f(\theta_A+\theta_B)>\\
&=&\sum\limits_{t=1}^{T+1}\frac{1}{2\eta}(\|(\theta_A+\theta_B)^{(t+1)}-\theta^{*}\|^2-\|(\theta_A+\theta_B)^{(t)}-\theta^{*}\|^2\\
&&+\eta^2\|\nabla f(\theta_A+\theta_B)\|^{2})\\
&=&\frac{1}{2\eta}(\|(\theta_A+\theta_B)^{(T+1)}-\theta^{*}\|^2-\|(\theta_A+\theta_B)^{(1)}-\theta^{*}\|^2)\\
&&+\frac{\eta}{2}\sum\limits_{t=1 }^{T}\|\nabla f(\theta_A+\theta_B)\|^{2}\\
&\leq&\frac{1}{2\eta}(\|(\theta_A+\theta_B)^{(1)}-\theta^{*}\|^2)+\frac{\eta}{2}\sum\limits_{t=1 }^{T}\|\nabla f(\theta_A+\theta_B)\|^{2}\\
&\leq&\frac{1}{2\eta}\sigma^2+\frac{\eta}{2}TG^2.
\end{eqnarray*}

Applying the basic inequality,
\begin{eqnarray*}
\frac{1}{2\eta}\sigma^2+\frac{\eta}{2}TG^2\leq G\sigma.
\end{eqnarray*}

Thus,
\begin{eqnarray*}
E[f(\overline{\theta_A+\theta_B})]-f(\theta^{*})\leq \frac{G\sigma}{\sqrt{T}}.
\end{eqnarray*}

Let $E[f(\overline{\theta_A+\theta_B})]-f(\theta^{*})\leq\epsilon$,
\begin{eqnarray*}
E[f(\overline{\theta_A+\theta_B})]-f(\theta^{*})\leq \frac{G\sigma}{\sqrt{T}}\leq \epsilon.
\end{eqnarray*}

Then,
\begin{eqnarray*}
T\leq \frac{G^2\sigma^2}{\epsilon^{2}}.
\end{eqnarray*}

\subsection{Convergence of MiniSGD-BPFL}
Definition 1. Let $b\in\{1,2,\cdots,n\}$, define the gradient variance of MiniSGD as
\begin{eqnarray}
\sigma_{b}^{*}\xlongequal{def}\inf\limits_{x^{*}\in argmin f} \nu[\nabla f_B(x^{*})].
\end{eqnarray}

Definition 2. Let $b\in\{1,2,\cdots,n\}$. $\forall \theta_1, \theta_2\in \mathbb{R} $, if function $f(\cdot)$ satisfies (\ref{lm}), then $f(\cdot)$ is said to be $L_b$-smooth under the expected definition.
\begin{eqnarray}\label{lm}
\nonumber
&&\frac{1}{2L_b}E[\|\nabla f_B(\theta_2)-\nabla f_B(\theta_1)\|^2]\\
&&\leq f(\theta_2)-f(\theta_1)-<\nabla f(\theta_1),\theta_2-\theta_1>.
\end{eqnarray}

Lemma 1. Assume that $f(\cdot)$ satisfies convexity and L-smoothness, then the $L_b$ under the expectation definition of $f(\cdot)$ is

\begin{eqnarray}\label{lb}
L_b=\frac{n(b-1)}{b(n-1)}L+\frac{n-b}{b(n-1)}L_{max}.
\end{eqnarray}
The gradient of PBFL-MiniSGD can be calculated by
\begin{eqnarray}
\sigma_{b}^{*}=\frac{n-b}{b(n-1)}\sigma_{f}^{*}.
\end{eqnarray}

The following discusses the expected smoothing constant $L_b$ and mini-batch gradient noise $\sigma_{b}^{*}$, when $b=1$, MiniSGD degenerate into SGD ($L_b=L_{max},\sigma_{b}^{*}=\sigma_{f}^{*}$). In this case, the convergence proof of MiniSGD equivalent to the proof of SGD.

Lemma 2. Assume that $f(\cdot)$ satisfies convexity and L-smoothness, we have
\begin{eqnarray}
E[\|\nabla f_B(\theta)\|^2]\leq4L_b(f(\theta)-\inf f)+2\sigma_{b}^{*}.
\end{eqnarray}

Theorem 2. Assume that $f(\cdot)$ satisfies convexity and L-smoothness, let $(\theta^t)_{t\in \mathbb{N}}$ is the sequence produced by MiniSGD, if the step size of each $\theta$ satisfies $0<\gamma_t\leq\frac{1}{4L_b}$, then for $T\geq1, \theta^{*}\in argmin f$ and $\overline{\theta_A+\theta_B}^{T}=\frac{1}{\sum_{t=0}^{T-1}\gamma_t}\sum_{t=0}^{T-1}\gamma_t\theta_A+\theta_A^{t}$,

\begin{eqnarray}
E[f(\overline{\theta_A+\theta_B}^{T})-\inf f]\leq\frac{\|\theta_A+\theta_B^{0}-\theta^{*}\|}{\sum_{t=0}^{T-1}\gamma_t}
+\frac{2\sigma_{b}^{*}\sum_{t=0}^{T-1}\gamma_t^{2}}{\sum_{t=0}^{T-1}\gamma_t}.
\end{eqnarray}

%The detailed proof of Theorem 2 is provided in Appendix B, which is further supported by the experimental results in Section 6.
{\bf Proof.}
Let $\theta^{*}\in argmin f$, then, $\sigma_{b}^{*}=\nu[\nabla f_B(x^{*})]$. The $L_2$ norm of the sequence $\|\theta_A+\theta_B^{t+1}-\theta^{*}\|$ is obtained as follows

\begin{eqnarray*}
\|\theta_A+\theta_B^{t+1}-\theta^{*}\|^2
&=&\|\theta_A+\theta_B^{t}-\gamma_t\nabla f_{B}(x^t)-\theta^{*}\|^2\\
&=&\|\theta_A+\theta_B^{t}-\theta^{*}\|^2\\
&&-2\gamma_t<\nabla f_{B}(\theta_A+\theta_B^{t}),\theta_A+\theta_B^{t}-\theta^{*}>\\
&&+\gamma_t^2\|\nabla f_{B}(\theta_A+\theta_B^{t})\|^2
\end{eqnarray*}

Using Lemma 1 and Lemma 2,
\begin{eqnarray*}
&&E[\|\theta_A+\theta_B^{t+1}-\theta^{*}\|^2|x^t]\\
&=&\|\theta_A+\theta_B^{t}-\theta^{*}\|^2-2\gamma_t<\nabla f_{B}(\theta_A+\theta_B^{t}),\theta_A+\theta_B^{t}-\theta^{*}>\\
&&+\gamma_t^2E[\|\nabla f_{B}(\theta_A+\theta_B^{t})\|^2|\theta_A+\theta_B^{t}]\\
&\leq&\|\theta_A+\theta_B^{t}-\theta^{*}\|^2-2\gamma_t(f(\theta_A+\theta_B^{t}-\inf f))\\
&&+\gamma_t^2E[\|\nabla f_{B}
(\theta_A+\theta_B^{t})\|^2|\theta_A+\theta_B^{t}]\\
&\leq&\|\theta_A+\theta_B^{t}-\theta^{*}\|^2-2\gamma_t(2\gamma_tL_b-1)(f(\theta_A+\theta_B^{t}-\inf f))+2\gamma_t^2\sigma_{b}^{*}\\
&\leq&\|\theta_A+\theta_B^{t}-\theta^{*}\|^2-\gamma_t(f(\theta_A+\theta_B^{t})-\inf f)+2\gamma_t^2\sigma_{b}^{*}
\end{eqnarray*}

The penultimate inequality applies the stride assumption $\gamma_t\leq\frac{1}{4\mathfrak{L}_b}$. Rearranging and moving terms from the above equation yields,
\begin{eqnarray*}
&&\gamma_tE[f(\theta_A+\theta_B^{t})-\inf f]\\
&\leq& E[\|\theta_A+\theta_B^{t}-\theta^{*}\|^2]-E[\|\theta_A+\theta_B^{t+1}-\theta^{*}\|^2]+2\gamma_t^2\sigma_{b}^{*}
\end{eqnarray*}

Sum up the above formula,
\begin{eqnarray*}
&&\sum\limits_{t=0}^{T-1}\gamma_tE[f(\theta_A+\theta_B^{t})-\inf f]\\
&\leq& E[\|\theta_A+\theta_B^{0}-\theta^{*}\|^2]-E[\|\theta_A+\theta_B^{T}-\theta^{*}\|^2]\\
&+&2\sigma_{b}^{*}\sum\limits_{t=0}^{T-1}\gamma_t^2
\end{eqnarray*}

Since $E[\|\theta_A+\theta_B^{T}-\theta^{*}\|^2]\geq0$, dividing both sides of the above inequality by $\sum\limits_{t=0}^{T-1}\gamma_t^2$ yields
\begin{eqnarray*}
&&\frac{1}{\sum\limits_{t=0}^{T-1}\gamma_t}
\sum\limits_{t=0}^{T-1}\gamma_tE[f(\theta_A+\theta_B^{t})-\inf f]\\
&\leq& \frac{\|\theta_A+\theta_B^{0}-\theta^{*}\|^2}{\sum\limits_{t=0}^{T-1}\gamma_t}
+\frac{2\sigma_{b}^{*}\sum\limits_{t=0}^{T-1}\gamma_t^2}{\sum\limits_{t=0}^{T-1}\gamma_t}
\end{eqnarray*}

Given $\overline{\theta_A+\theta_B}^{T}=\frac{1}{\sum_{t=0}^{T-1}\gamma_t}\sum_{t=0}^{T-1}\gamma_t\theta^{t}$,
applying Jensen's inequality to $f$ yields

\begin{eqnarray*}
&&E[f(\overline{\theta_A+\theta_B}^{T})-\inf f]\\
&\leq& E[\frac{1}{\sum_{t=0}^{T-1}\gamma_t}\sum_{t=0}^{T-1}\gamma_t
(f(\theta_A+\theta_B^{t})-\inf f)]\\
&\leq&\frac{\|\theta_A+\theta_B^{0}-\theta^{*}\|}{\sum_{t=0}^{T-1}\gamma_t}
+\frac{2\sigma_{b}^{*}\sum_{t=0}^{T-1}\gamma_t^{2}}{\sum_{t=0}^{T-1}\gamma_t}
\end{eqnarray*}


\section{Experiment and analysis}

\subsection{Data and experimental design}

This paper uses the German credit dataset to train and validate the proposed BPFL method, which is frequently utilized in credit evaluation research\citep{Yang:25, Patel:26, Khalili:27}. The dataset contains 1000 samples and 20 features, including status\_account, duration, and credit\_history, etc (7 continuous and 13 categorical). During the data preprocessing, WOE encoding\citep{Thomas:28} is used for nonnumerical variables. After encoding and feature selection, as shown in Figure \ref{FX}, only 14 features remain.

To address the dual challenges of high ciphertext computation costs and blockchain communication overhead in vertical federated learning, this paper utilizes the XGBoost model to pre-train a portion of the dataset and obtain feature importance ranking. As shown in Figure \ref{FX}, features such as age, loan duration, and property have a significant impact on the model's accuracy. Based on this, ablation experiments were conducted by sequentially removing the less important features to observe changes in the model's performance. As illustrated in Figure \ref{FXL}, with the reduction in the number of features, particularly the removal of less important ones, the model's loss value significantly decreases, and the rate of decline accelerates. This indicates that feature selection can reduce computational overhead and improve the training efficiency and performance of the model. Most importantly, retaining the most critical features (such as age, loan duration, and property) significantly enhances the model's accuracy and efficiency.

\begin{figure*}[!h]
    \centering
    \subfigure[XGBoost feature selection]{
     \includegraphics[width=8.25cm, height=5.75cm]{Figure/FX.jpg}
    \label{FX}
    }
    \quad
    \subfigure[Loss performance under different features]{
    \includegraphics[width=8.25cm, height=5.75cm]{Figure/FXL.png}
    \label{FXL}
    }
    \caption{XGBoost feature selection and loss performance}
    \label{FX-L}
\end{figure*}

For the experiment, this study randomly split the original dataset vertically based on features into two parts, representing Bank A and Bank B. Each bank holds an equal number of different features. And the hardware and software environments used to implement the prototype system are shown in Table \ref{tb hs}.
\begin{table}[!hb]
\caption{\bf Hardware and software environments.}
\begin{center}
\begin{tabular}{ll}
 \hline
{\bf Hardware}  \\
\hline
CPU    &	Intel 8336C \\
GPU    &    GeForce RTX4090(24g)\\
Memory &    256 GB\\
 \hline

{\bf Software}  \\
\hline
OS                 &	Ubuntu 22.04 \\
Golang             &	v1.90 \\
Python             &    3.11 \\
Pytorch            &	2.0.0 \\
cuda               &	11.8 \\
Cliper             &	v0.5.0 \\
Hyperledger fabric &    2.4.1\\
\hline
\end{tabular}
\end{center}
 \label{tb hs}
\end{table}

\subsection{Prototype system}

Figure \ref{HER} shows the activity status of the overall underlying blockchain network by the Hyperledger Expolrer, which includes 12 blocks, 12 transactions, 4 nodes, and 1 chaincode. Figure \ref{PBFLM} shows the visualization of blockchain node operations monitoring using the Grafana plugin. The specific monitoring metrics include block processing speed, the number of successful transactions per second, the number of failed transactions, and the total number of transactions in the channel.

\begin{figure*}[!ht]
    \centering
    \subfigure[Hyperledger Explorer(DASHBOARD)]{
     \includegraphics[width=8.25cm, height=5.75cm]{Figure/HER.png}
    \label{HER}
    }
    \quad
    \subfigure[Operations Monitoring]{
    \includegraphics[width=8.25cm, height=5.75cm]{Figure/PBFLM1.png}
    \label{PBFLM}
    }
    \caption{Blockchain Network Visualization and Operations Monitoring}
    \label{DM}
\end{figure*}


\subsection{Performance of BC-PSI}
To analyse the feasibility of the proposed BC-PSI, the experiment uses Diffie-Hellman-based PSI as a benchmark method and takes the time cost as the evaluation metric to quantify the processing time of completing the co-sample ID comparison among all participants. The experimental results are presented in Figure \ref{PSI}. For different datasets, the execution times of the proposed BC-PSI are slightly better than those of the Diffie-Hellman-based PSI in terms of performing co-sample alignment.

\begin{figure}[!ht]
\centering
\includegraphics[width=8.25cm, height=5.25cm]{Figure/PSI.jpg}
\caption{Performance comparison between BC-PSI and Diffie-Hellman-based PSI.}
\label{PSI}
\end{figure}

\subsection{Performance of BPFL}

As previously indicated, the CKKS-based PBFL scheme primarily consists of key generation, plaintext encryption, ciphertext operations, and ciphertext decryption. The BFV\citep{Brakerski:01} and Paillier\citep{Paillier:01} homomorphic encryption are chosen as the benchmark methods for PBFL. Figure \ref{CKG} illustrates that CKKS significantly outperforms the BFV and Paillier schemes in terms of key generation time. Figures \ref{CEN} and \ref{CDE} show that CKKS and BFV schemes outperformed Paillier schemes for both plaintext encryption and ciphertext decryption at different vector lengths. Compared to the Paillier scheme, Figure \ref{CAO} illustrates that CKKS and BFV ciphertext aggregation costs much less time with vector length increasing. Specifically, CKKS performs slightly better than BFV. Therefore, we can infer that the proposed CKKS-based PBFL methodology is significantly superior in privacy-federated collaborative modelling.

\begin{figure*}[!h]
    \centering
    \subfigure[Key-generation costs under different homomorphic encryption]{
    \includegraphics[width=8.25cm, height=5.75cm]{Figure/CKG.jpg}
    \label{CKG}
    }
    \quad
    \subfigure[Encryption costs under different homomorphic encryption]{
     \includegraphics[width=8.25cm, height=5.75cm]{Figure/CEN.jpg}
    \label{CEN}
    }
    \centering
    \subfigure[Aggregation costs under different homomorphic encryption]{
     \includegraphics[width=8.25cm, height=5.75cm]{Figure/CAO.jpg}
    \label{CAO}
    }
    \quad
    \subfigure[Decryption costs under different homomorphic encryption]{
    \includegraphics[width=8.25cm, height=5.75cm]{Figure/CDE.jpg}
    \label{CDE}
    }
    \caption{Performance comparison between CKKS, BFV and Paillier homomorphic encryption.}
    \label{RR1}
\end{figure*}

Figure \ref{EAD} shows the time costs for each stage of CKKS-based PBFL in Section 5.2.  As illustrated, adopting the CKKS scheme results in shorter durations for each stage of federated training. Whether it's participant A and B encrypting data, performing aggregation operations on ciphertext data, or participant C decrypting ciphertexts, the time consumption is relatively low.  This is mainly because CKKS supports more complex operations beyond simple additions and multiplications, and it can control the growth of ciphertexts during these operations. Additionally, CKKS allows for computations on encrypted data with approximate results, which is beneficial for applications involving real or complex numbers. Therefore, CKKS homomorphic encryption is well-suited for scenarios requiring efficient computations on encrypted data, particularly those involving large-scale numerical operations or complex computations.

\begin{figure}[!h]
\centering
\includegraphics[width=8.5cm, height=6.25cm]{Figure/EAD.jpg}
\caption{Time costs of CKKS-based BPFL.}
\label{EAD}
\end{figure}

To further evaluate the discrimination performance of the proposed BPFL method, accuracy, precision, recall, and F1 score are utilized to compare PBFL method (i.e., the real scenario) and CFL method (i.e., ideal scenario, that is, multi-source data can be easily integrated integrated learning by a central node), which respectively measure the overall accuracy of the model, the accuracy of positive class predictions, the detection ability of positive class samples, and the balance between precision and recall. As shown in Table \ref{PC}, despite the slight difference in the BPFL performance relative to that of the CFL method, the BPFL approaches the CFL method for each performance metric. Considering the privacy requirements in specific scenarios and the computational performance of homomorphic encryption, the discrimination performance of BPFL is still acceptable.

\begin{table*}[!ht]
\caption{Performance comparison between the CFL and BPFL methods}
\begin{center}
\begin{tabular}{lllll}

\hline
             & \multicolumn{3}{c}{Measures} \\ \cline{2-5}

Model         &Accuracy   & Precision   & Recall    &F1 Score \\

\hline
CFL           &  0.745     &0.60        & 0.45      &0.51\\

BPFL          &0.720       &0.56        &0.42       &0.48 \\
\hline
\end{tabular}
\end{center}
\label{PC}
\end{table*}

Moreover, Figure \ref{RR} shows that the AUC and Loss curves of the BPFL and CFL methods. AUC curve is primarily used for measuring the performance of classification models, especially evaluating binary classifiers. The larger the AUC value, the stronger the model's discriminative ability. Loss curve is taken as the evaluation metric to evaluate the convergence and performance of BPFL and CFL methods during the training process. The lower the loss is, the better the model's performance on the training data. As illustrated, our proposed BPFL method can perform similarly to the ideal scenario (i.e., CFL). The experimental results also further supported the convergence of BPFL in Section 6.

\begin{figure*}[!h]
    \centering
    \subfigure[The AUC curves of the BPFL and CFL methods]{
     \includegraphics[width=8.25cm, height=5.75cm]{Figure/AUC.png}
    \label{AUC}
    }
    \quad
    \subfigure[The Precision-Recall of the BPFL and CFL methods]{
    \includegraphics[width=8.25cm, height=5.75cm]{Figure/Loss.png}
    \label{LOSS}
    }
    \caption{AUC and Loss curves}
    \label{RR}
\end{figure*}
\subsection{System performance}

Cliper is used for conducting stress testing on ciphertext queries, write, and delete smart contracts to analyze the relationship between ciphertext length and system performance. As shown in Figure \ref{TPS}, the performance of ciphertext query and deletion operations are superior to that of write operation. However, as the ciphertext size increases, the throughput of these operations all decrease to varying degrees. For example, when the ciphertext size reaches 1.0KB, the write operation throughput drops to 387; when the ciphertext size reaches 2.2KB, the throughput decreases to 316; and when the ciphertext size reaches 3.4KB, the throughput decreases to 236. This indicates that the throughput of the blockchain decreases as the ciphertext length increases. Figure \ref{AL} shows the average latency for ciphertext queries, writes, and deletions on the blockchain network. The results indicate that the average latency increases with the ciphertext size increasing.. When the ciphertext size reaches 2KB, the average latency increases from 0.03 seconds to 0.08 seconds, and then stabilizes around 0.07 seconds. This demonstrates that the increase in ciphertext size also leads to an increase in the average latency of blockchain transactions.

However, as can be seen in Figure \ref{TPS}, the throughput of the lowest query operation is 300-tps, and even the write operation has 240-tps with an average delay of about 1 second. This means that the platform can process at least 300 query operations or 240 write transactions in one second under a test environment. Therefore, the platform has reasonable performance in querying data and writing data in terms of results. Indeed, to further alleviate the ledger storage and write pressure on blockchain, ciphertext can be compressed before being added to the chain, with decompression operations performed locally at the federated client nodes. This will help improve the performance of the blockchain network.
\begin{figure*}[!ht]
    \centering
    \subfigure[Throughput for Query-Write-Delete]{
     \includegraphics[width=8.25cm, height=5.75cm]{Figure/TPS.png}
    \label{TPS}
    }
    \quad
    \subfigure[Average Latencey for Query-Write-Delete]{
    \includegraphics[width=8.25cm, height=5.75cm]{Figure/AL.png}
    \label{AL}
    }
    \caption{Caliper Pressure test}
    \label{CPT}
\end{figure*}
\subsection{Global explanations for BPFL}

Without disrupting the existing training, the model incorporates a smart contract for the automatic computation of SHAP values through a blockchain network, aiming to assess the feature contributions of nodes in each training iteration. In Figure \ref{SP}, the x-axis represents the feature values of the samples, while the y-axis represents different features. The position and colour of each point reflect the contributions of diverse dimensions of the data samples to the model output, with the colour depth intuitively indicating the magnitude of the contribution. Some instances of red points appearing on the negative side of the axis may result from the offsetting effects between negative impacts of certain features and positive impacts of others, leading to an overall negative expected value. Figure \ref{AM} shows the importance ranking of features, where red denotes contributions to positive class samples (class 0), and blue signifies contributions to negative class samples (class 1). The length of the bars in the chart reflects the contribution of each feature to the overall model. Notably, features such as ``purpose," ``status\_account," and ``duration" had significant influences on the model.

\begin{figure*}[!hb]
    \centering
    \subfigure[Total SHAP values]{
    \includegraphics[width=8.25cm, height=5.75cm]{Figure/SP.png}
    \label{SP}
    }
    \quad
    \subfigure[Average impact on model output magnitude]{
     \includegraphics[width=8.25cm, height=5.75cm]{Figure/AM.png}
    \label{AM}
    }
    \caption{Global model feature contribution}
    \label{GFC}
\end{figure*}

\section*{Acknowledgment}
This research was supported by the National Natural Science Foundation of China (No.71850012, 71790593), National Social Science Fund of China (No. 19AZD014), Major Special Projects of the Department of Science and Technology of Hunan Province (No. 2018GK1020).

%% Loading bibliography style file
%\bibliographystyle{model1-num-names}
\bibliographystyle{cas-model2-names}

% Loading bibliography database
\bibliography{cas-refs}

% Biography
% \bio{}
% % Here goes the biography details.
% \endbio

% \bio{pic1}
% % Here goes the biography details.
% \endbio

\end{document}