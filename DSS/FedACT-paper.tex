%% FedACT Paper for Decision Support Systems
%% Final Version - Streamlined and Academic Style

\documentclass[preprint,12pt]{elsarticle}

\usepackage{amssymb,amsmath}
\usepackage{booktabs,multirow}
\usepackage{graphicx,float}
\usepackage{enumitem}
\usepackage{algorithm,algorithmic}
\usepackage{xcolor}

\journal{Decision Support Systems}

\begin{document}

\begin{frontmatter}

\title{FedACT: Byzantine-Resilient Federated Learning with Explicit Attack Detection for Credit Scoring}

\author[ucas,pboc]{Dengjia Li}
\author[ucas]{Han Qiao}
\author[pboc]{Chen Yang}
\author[sdut,hunan]{Yuncheng Qiao\corref{cor1}}
\ead{qiaoyc@sdut.edu.cn}

\cortext[cor1]{Corresponding author}

\affiliation[ucas]{organization={School of Economics and Management, University of Chinese Academy of Sciences},
            city={Beijing}, postcode={100190}, country={China}}
\affiliation[pboc]{organization={China National Clearing Center, The People's Bank of China},
            city={Beijing}, postcode={100048}, country={China}}
\affiliation[sdut]{organization={Business School, Shandong University of Technology},
            city={Zibo}, postcode={255000}, country={China}}
\affiliation[hunan]{organization={Key Laboratory of High-Performance Distributed Ledger and Digital Finance, Ministry of Education},
            city={Changsha}, postcode={410082}, country={China}}

\begin{abstract}
Federated learning enables privacy-preserving collaborative credit scoring, yet existing Byzantine-resilient aggregation methods focus solely on maintaining model accuracy without identifying malicious participants. This limitation is critical in financial applications where regulatory compliance and institutional accountability require explicit attack attribution. We propose FedACT, a framework that prioritizes explicit Byzantine detection over implicit robustness. FedACT employs a three-stage pipeline: autoencoder-based anomaly scoring with adaptive thresholding partitions gradients into normal, uncertain, and anomalous zones; diversity-constrained committee voting resolves borderline cases while accommodating legitimate data heterogeneity; and reputation-weighted aggregation with Merkle-tree evidence logging provides long-term incentives and tamper-evident audit trails. Experiments on real-world credit datasets demonstrate that FedACT achieves 89.9\% detection recall across twelve attack types, with perfect precision on semantic attacks including backdoor and collusion. While traditional robust aggregators achieve marginally higher model accuracy by silently filtering outliers, FedACT uniquely provides the detection capability and auditability essential for regulated financial environments.
\end{abstract}

\begin{highlights}
\item A Byzantine detection framework providing explicit attack identification rather than implicit filtering for federated credit scoring.
\item Three-zone classification with committee voting accommodates data heterogeneity while maintaining high detection recall.
\item Reputation system with Merkle-tree logging enables institutional accountability and regulatory audit compliance.
\item Achieves 89.9\% detection recall with perfect precision on semantic attacks across heterogeneous data scenarios.
\end{highlights}

\begin{keyword}
Federated learning \sep Byzantine detection \sep Credit scoring \sep Anomaly detection \sep Auditability
\end{keyword}

\end{frontmatter}

%% ============================================================================
%% INTRODUCTION
%% ============================================================================
\section{Introduction}
\label{sec:intro}

Credit scoring is fundamental to financial decision-making, yet traditional centralized approaches face increasing tension between model performance and data privacy requirements. Privacy regulations such as GDPR and China's Personal Information Protection Law impose strict constraints on cross-institutional data sharing, motivating the adoption of federated learning for collaborative credit modeling \citep{Yang:2019:FL,Long:2020}. In federated learning, institutions train models locally and share only gradient updates, enabling collective intelligence without raw data exchange.

However, the distributed nature of federated learning introduces vulnerability to Byzantine attacks, where malicious participants submit corrupted gradient updates to degrade model performance or inject backdoors \citep{Blanchard:2017}. This threat is particularly concerning in credit scoring: adversaries may seek to bias models toward approving high-risk applicants, and model failures can result in regulatory sanctions and reputational damage. The challenge is amplified by data heterogeneity inherent in cross-silo settings, where institutions serve distinct customer segments with non-IID data distributions that can mask or mimic malicious behavior \citep{Li:2022:FedNonIID}.

Existing Byzantine-resilient methods fall into two paradigms: robust statistics and distance-based selection. Robust aggregators such as coordinate-wise median \citep{Yin:2018}, trimmed mean, and geometric median \citep{Pillutla:2019} replace vulnerable averaging with estimators that tolerate outliers. Distance-based methods including Krum \citep{Blanchard:2017} and Bulyan \citep{ElMhamdi:2018} identify and exclude gradients far from the majority. While these approaches can maintain model accuracy under attack, they share a fundamental limitation: they provide no explicit identification of malicious participants. Outliers are silently filtered without attribution, leaving institutions unable to determine whether anomalies stem from attacks or legitimate heterogeneity, and providing no basis for accountability or regulatory reporting.

This implicit robustness paradigm is insufficient for regulated financial environments. Credit scoring systems require audit trails documenting how decisions were made and who participated in model training. When a defense mechanism silently excludes a gradient, there is no record of whether an attack occurred, which institution was responsible, or what evidence supported the exclusion. This opacity conflicts with regulatory expectations for explainability and accountability in automated financial decision-making.

We propose FedACT, a framework that shifts from implicit robustness to explicit detection. Rather than silently filtering outliers, FedACT explicitly identifies anomalous gradients, classifies them with calibrated uncertainty, and maintains tamper-evident records of all detection decisions. The framework comprises three stages: an autoencoder learns normal gradient manifolds and computes dual-metric anomaly scores, with MAD-based adaptive thresholding partitioning gradients into normal, uncertain, and anomalous zones; a diversity-constrained committee of dissimilar normal clients votes on borderline cases, reducing false positives from legitimate heterogeneity; and verified gradients aggregate via reputation-weighted optimization, with a dynamic reputation system providing long-term incentives and Merkle-tree logging ensuring auditability.

This design reflects a deliberate trade-off. Traditional robust aggregators achieve slightly higher model accuracy by aggressively filtering any gradient that deviates from the majority, but cannot distinguish attacks from heterogeneity. FedACT accepts marginally lower accuracy in exchange for explicit detection capability, enabling institutions to identify malicious participants, accumulate evidence over time through the reputation system, and provide auditors with verifiable records. In financial applications where accountability matters as much as accuracy, this trade-off is appropriate.

Our contributions are: (1) a three-stage Byzantine detection framework providing explicit attack identification with calibrated uncertainty for heterogeneous federated learning; (2) a diversity-constrained committee mechanism that reduces false positives from legitimate data heterogeneity; (3) a reputation system with asymmetric updates and Merkle-tree evidence logging for institutional accountability; and (4) comprehensive evaluation demonstrating 89.9\% detection recall with perfect precision on semantic attacks.


%% ============================================================================
%% RELATED WORK
%% ============================================================================
\section{Related Work}
\label{sec:related}

\subsection{Byzantine Attacks in Federated Learning}

Byzantine attacks in distributed systems date to the foundational work of Lamport et al. \citep{Lamport:1982}, who characterized the challenge of reaching consensus when participants may behave arbitrarily. In federated learning, these attacks have evolved from simple perturbations to sophisticated optimization-based strategies. Basic attacks include sign-flipping, Gaussian noise injection, and gradient scaling, which can degrade convergence when defenses assume honest majorities \citep{Fang:2020}. Optimization-based attacks explicitly craft updates to evade detection: ALIE generates perturbations within benign distribution tails \citep{Baruch:2019}, IPM manipulates inner products to fool distance-based methods \citep{Xie:2020}, and MinMax solves constrained optimization to maximize damage while satisfying detectability constraints \citep{Shejwalkar:2021}. Semantic attacks achieve malicious objectives through gradients that may appear statistically normal, including backdoor injection \citep{Bagdasaryan:2020}, label corruption, and coordinated collusion among multiple malicious clients.

\subsection{Byzantine-Resilient Aggregation}

Defenses against Byzantine attacks employ several approaches. Robust statistics methods replace averaging with estimators having high breakdown points: coordinate-wise median provides 50\% breakdown \citep{Yin:2018}, trimmed mean discards extreme values, and geometric median minimizes sum of distances \citep{Pillutla:2019}. These methods assume honest gradients cluster tightly, an assumption violated under heterogeneity. Distance-based selection methods identify outliers through pairwise distances: Krum selects the gradient with minimum distance to its nearest neighbors \citep{Blanchard:2017}, and Bulyan combines selection with trimmed aggregation \citep{ElMhamdi:2018}. Trust-anchored methods such as FLTrust leverage server-held clean data to compute trust scores \citep{Cao:2021}, though requiring server-side data may be inappropriate in privacy-sensitive domains. Learning-based approaches using autoencoders or isolation forests can capture complex attack patterns but typically make binary decisions without handling the uncertainty inherent in heterogeneous settings \citep{Li:2023:AutoFL}.

A critical gap in existing methods is the absence of explicit detection capability. All approaches above focus on maintaining model accuracy by filtering outliers, but none provides attribution of which participants are malicious, evidence supporting detection decisions, or audit trails for regulatory compliance. This implicit robustness paradigm is insufficient for financial applications requiring accountability.

\subsection{Federated Learning for Credit Scoring}

Credit scoring has evolved from logistic regression to gradient boosting and neural networks \citep{Lessmann:2015}, with federated approaches emerging to address privacy constraints. Yang et al. \citep{Yang:2024} propose explainable federated learning with blockchain for credit modeling, addressing interpretability requirements. Vertical federated learning enables collaboration when institutions hold different features for overlapping customers \citep{Chen:2022}. However, Byzantine resilience in federated credit scoring remains underexplored, with existing work assuming honest participation and leaving systems vulnerable to strategic manipulation.


%% ============================================================================
%% PROBLEM FORMULATION
%% ============================================================================
\section{Problem Formulation}
\label{sec:problem}

Consider $N$ financial institutions collaboratively training a credit scoring model $\mathbf{w} \in \mathbb{R}^d$. Each client $i$ holds private data $\mathcal{D}_i = \{(\mathbf{x}_j, y_j)\}_{j=1}^{n_i}$ where $\mathbf{x}_j$ represents applicant features and $y_j \in \{0,1\}$ indicates default status. The federated objective minimizes
\begin{equation}
F(\mathbf{w}) = \sum_{i=1}^{N} \frac{n_i}{n} F_i(\mathbf{w}), \quad F_i(\mathbf{w}) = \frac{1}{n_i} \sum_{j=1}^{n_i} \ell(\mathbf{w}; \mathbf{x}_j, y_j)
\end{equation}
where $\ell$ is binary cross-entropy and $n = \sum_i n_i$. Training proceeds in rounds: the server broadcasts $\mathbf{w}^{(t)}$, clients perform local updates, and gradients $\mathbf{g}_i^{(t)} = \mathbf{w}^{(t)} - \mathbf{w}_i^{(t)}$ are aggregated.

We assume $M < N/2$ Byzantine clients with white-box knowledge of the defense mechanism, ability to submit arbitrary gradients, and potential for coordination. We evaluate against twelve attacks: three basic (sign-flip, Gaussian, scaling), five optimization-based (little, ALIE, IPM, MinMax, trim), and four semantic (label-flip, backdoor, free-rider, collision). Data heterogeneity includes IID baseline, label skew via Dirichlet allocation ($\alpha=0.5$), feature distribution shifts, and power-law quantity skew.


%% ============================================================================
%% THE FedACT FRAMEWORK
%% ============================================================================
\section{The FedACT Framework}
\label{sec:method}

FedACT defends against Byzantine attacks through three integrated stages, each implemented as a distinct algorithm. We present each stage with its algorithm embedded in the corresponding subsection.

\subsection{Stage 1: Autoencoder-Based Anomaly Detection}

The first stage learns a low-dimensional representation of benign gradient distributions using an autoencoder trained on historical normal gradients. For each incoming gradient $\mathbf{g}_i$, we compute two complementary metrics: reconstruction error $e_i = \|\mathbf{g}_i - \psi(\phi(\mathbf{g}_i))\|^2$ captures deviation from the learned manifold, while latent deviation $d_i = \|\phi(\mathbf{g}_i) - \boldsymbol{\mu}_z\|$ measures distance from the normal gradient centroid in latent space. These metrics are max-normalized and combined as $a_i = 0.7\tilde{e}_i + 0.3\tilde{d}_i$, weighting reconstruction error more heavily as it better captures structural violations.

Rather than making binary decisions, we partition gradients into three zones using MAD-based adaptive thresholding. The threshold $\tau = \text{med}(\mathbf{a}) + 2.5 \cdot 1.4826 \cdot \text{MAD}(\mathbf{a})$ adapts to the score distribution each round. Gradients with scores below $0.7\tau$ are classified as normal, those above $1.5\tau$ as anomalous, and intermediate scores fall into an uncertain zone requiring further adjudication. This three-zone approach acknowledges the fundamental difficulty of distinguishing sophisticated attacks from legitimate heterogeneity based on anomaly scores alone.

\begin{algorithm}[h]
\caption{Autoencoder-Based Three-Zone Detection}
\label{alg:autoencoder}
\begin{algorithmic}[1]
\REQUIRE Gradients $\mathbf{G} = \{\mathbf{g}_1,\ldots,\mathbf{g}_N\}$, autoencoder $(\phi, \psi)$
\ENSURE Normal set $\mathcal{N}$, uncertain set $\mathcal{U}$, anomalous set $\mathcal{A}$
\FOR{each $\mathbf{g}_i \in \mathbf{G}$}
    \STATE $e_i \gets \|\mathbf{g}_i - \psi(\phi(\mathbf{g}_i))\|^2$
    \STATE $d_i \gets \|\phi(\mathbf{g}_i) - \boldsymbol{\mu}_z\|$
    \STATE $a_i \gets 0.7 \cdot \tilde{e}_i + 0.3 \cdot \tilde{d}_i$
\ENDFOR
\STATE $\tau \gets \text{med}(\mathbf{a}) + 2.5 \cdot 1.4826 \cdot \text{MAD}(\mathbf{a})$
\STATE $\mathcal{N} \gets \{i: a_i < 0.7\tau\}$
\STATE $\mathcal{U} \gets \{i: 0.7\tau \leq a_i < 1.5\tau\}$
\STATE $\mathcal{A} \gets \{i: a_i \geq 1.5\tau\}$
\RETURN $\mathcal{N}, \mathcal{U}, \mathcal{A}$
\end{algorithmic}
\end{algorithm}

\subsection{Stage 2: Diversity-Constrained Committee Voting}

Borderline cases in the uncertain zone are resolved through committee voting. We select $K=5$ committee members from the normal set $\mathcal{N}$, maximizing diversity to ensure the committee represents varied but legitimate gradient directions. The first member has highest reputation; subsequent members are chosen to minimize maximum cosine similarity to already-selected members. Each committee member votes on whether an uncertain gradient is anomalous based on cosine similarity: if similarity falls below threshold $\gamma=0.3$, the member votes to flag the gradient. Majority voting determines the final classification.

This mechanism addresses a key limitation of single-threshold detection. Under heterogeneity, some legitimate gradients will have elevated anomaly scores simply because they represent minority data distributions. A diverse committee drawn from normal clients provides multiple reference points, reducing false positives while maintaining sensitivity to genuine attacks that deviate from all normal directions.

\begin{algorithm}[h]
\caption{Diversity-Constrained Committee Voting}
\label{alg:committee}
\begin{algorithmic}[1]
\REQUIRE Normal set $\mathcal{N}$, uncertain set $\mathcal{U}$, gradients $\{\mathbf{g}_i\}$, reputations $\{\rho_i\}$
\ENSURE Updated normal set $\mathcal{N}'$, anomalous set $\mathcal{A}'$
\STATE $c_1 \gets \arg\max_{i \in \mathcal{N}} \rho_i$
\STATE $\mathcal{C} \gets \{c_1\}$
\FOR{$k = 2$ to $K$}
    \STATE $c_k \gets \arg\min_{i \in \mathcal{N} \setminus \mathcal{C}} \max_{j \in \mathcal{C}} \cos(\mathbf{g}_i, \mathbf{g}_j)$
    \STATE $\mathcal{C} \gets \mathcal{C} \cup \{c_k\}$
\ENDFOR
\STATE $\mathcal{N}' \gets \mathcal{N}$, $\mathcal{A}' \gets \emptyset$
\FOR{each $u \in \mathcal{U}$}
    \STATE votes $\gets \sum_{c \in \mathcal{C}} \mathbf{1}[\cos(\mathbf{g}_u, \mathbf{g}_c) < 0.3]$
    \IF{votes$/K \leq 0.5$}
        \STATE $\mathcal{N}' \gets \mathcal{N}' \cup \{u\}$
    \ELSE
        \STATE $\mathcal{A}' \gets \mathcal{A}' \cup \{u\}$
    \ENDIF
\ENDFOR
\RETURN $\mathcal{N}'$, $\mathcal{A}'$
\end{algorithmic}
\end{algorithm}

\subsection{Stage 3: TLBO-Based Robust Aggregation with Reputation}

Verified gradients from $\mathcal{N}$ aggregate via Teaching-Learning-Based Optimization with reputation weighting. TLBO is a population-based metaheuristic that iteratively improves candidate solutions through two phases. In our adaptation, each gradient serves as a learner, and the optimization objective is to maximize model accuracy rather than gradient similarity.

The fitness function for each gradient is its contribution to test accuracy. We temporarily apply a gradient to the global model, evaluate accuracy on a validation subset, then restore the original parameters. In the teacher phase, the gradient achieving highest accuracy becomes the teacher, and other gradients move toward it. In the learner phase, pairs of gradients interact: each gradient moves toward better-performing peers and away from worse performers. After $T=10$ iterations, this process yields the gradient that maximizes model accuracy.

The reputation system provides long-term memory and incentives. Reputations $\rho_i \in [0.1, 2.0]$ update asymmetrically: normal clients receive additive rewards $\rho_i \gets \rho_i + 0.05\xi_i$ proportional to their alignment with consensus, while detected anomalies suffer multiplicative penalties $\rho_i \gets 0.7\rho_i$. This asymmetry means a single anomalous detection reduces reputation by 30\%, requiring approximately six honest rounds to recover. Persistent attackers accumulate reputation damage, progressively reducing their influence even if they occasionally evade detection.

All detection decisions are logged to a Merkle tree, creating a tamper-evident audit trail. Each entry records the round, client identities, anomaly scores, zone classifications, and reputation updates.

\begin{algorithm}[h]
\caption{TLBO Aggregation with Reputation Updates}
\label{alg:tlbo}
\begin{algorithmic}[1]
\REQUIRE Normal gradients $\{\mathbf{g}_i: i \in \mathcal{N}'\}$, reputations $\{\rho_i\}$, iterations $T$
\ENSURE Aggregated gradient $\mathbf{g}^*$, updated reputations $\{\rho_i'\}$
\STATE Initialize learners: $\mathcal{L} \gets \{\mathbf{g}_i: i \in \mathcal{N}'\}$
\FOR{$t = 1$ to $T$}
    \STATE \textit{// Compute fitness (accuracy) for each learner}
    \FOR{each $\mathbf{l}_i \in \mathcal{L}$}
        \STATE $f_i \gets \text{EvaluateAccuracy}(\mathbf{l}_i)$
    \ENDFOR
    \STATE \textit{// Teacher phase: learn from best accuracy}
    \STATE $\text{teacher} \gets \arg\max_{\mathbf{l} \in \mathcal{L}} f(\mathbf{l})$
    \STATE $\bar{\mathbf{l}} \gets \frac{1}{|\mathcal{L}|}\sum_{\mathbf{l} \in \mathcal{L}} \mathbf{l}$
    \STATE $TF \sim \text{Uniform}\{1, 2\}$
    \FOR{each $\mathbf{l}_i \in \mathcal{L}$}
        \STATE $\mathbf{l}_i' \gets \mathbf{l}_i + r(\text{teacher} - TF \cdot \bar{\mathbf{l}})$ where $r \sim \mathcal{U}(0,1)$
        \IF{$\text{EvaluateAccuracy}(\mathbf{l}_i') > f_i$}
            \STATE $\mathbf{l}_i \gets \mathbf{l}_i'$
        \ENDIF
    \ENDFOR
    \STATE \textit{// Learner phase: mutual learning based on accuracy}
    \FOR{each $\mathbf{l}_i \in \mathcal{L}$}
        \STATE $j \sim \text{Uniform}(\{k: k \neq i\})$
        \STATE $\mathbf{l}_i' \gets \mathbf{l}_i + r(\mathbf{l}_j - \mathbf{l}_i)$ if $f_j > f_i$, else $\mathbf{l}_i + r(\mathbf{l}_i - \mathbf{l}_j)$
        \IF{$\text{EvaluateAccuracy}(\mathbf{l}_i') > f_i$}
            \STATE $\mathbf{l}_i \gets \mathbf{l}_i'$
        \ENDIF
    \ENDFOR
\ENDFOR
\STATE $\mathbf{g}^* \gets \arg\max_{\mathbf{l} \in \mathcal{L}} \text{EvaluateAccuracy}(\mathbf{l})$
\STATE \textit{// Update reputations}
\FOR{each client $i$}
    \IF{$i \in \mathcal{N}'$}
        \STATE $\xi_i \gets (\cos(\mathbf{g}_i, \mathbf{g}^*)+1)/2$
        \STATE $\rho_i' \gets \min(\rho_i + 0.05\xi_i, 2.0)$
    \ELSE
        \STATE $\rho_i' \gets \max(\rho_i \times 0.7, 0.1)$
    \ENDIF
\ENDFOR
\RETURN $\mathbf{g}^*$, $\{\rho_i'\}$
\end{algorithmic}
\end{algorithm} The hash chain ensures that any modification to historical records is detectable, enabling auditors to verify the integrity of the detection history and supporting regulatory compliance requirements for financial model governance.


%% ============================================================================
%% EXPERIMENTS
%% ============================================================================
\section{Experiments}
\label{sec:experiments}

\subsection{Experimental Setup}

We evaluate on two credit scoring datasets: UCI Credit Card Default with 30,000 Taiwan credit card clients and 23 features \citep{Yeh:2009}, and Xinwang Bank with 50,000 loan applicants and 35 features from a Chinese commercial bank. Models are three-layer MLPs trained for 100 rounds with 5 local epochs across $N=10$ clients including $M=3$ attackers. Experiments repeat 3 times on NVIDIA RTX 4090 GPUs.

\subsection{Detection Performance}

Table~\ref{tab:detection} reports FedACT's detection performance across attack categories, averaged over both datasets and four heterogeneity scenarios.

\begin{table}[h]
\centering
\caption{Detection performance by attack category.}
\label{tab:detection}
\begin{tabular}{@{}llccc@{}}
\toprule
Category & Attack & Precision & Recall & F1 \\
\midrule
\multirow{3}{*}{Basic} & Sign-flip & 0.302 & 0.939 & 0.457 \\
& Gaussian & 0.301 & 0.955 & 0.458 \\
& Scaling & 0.328 & 0.764 & 0.457 \\
\midrule
\multirow{5}{*}{Optimization} & Little & 0.301 & 0.956 & 0.458 \\
& ALIE & 0.300 & 0.949 & 0.456 \\
& IPM & 0.431 & 0.925 & 0.582 \\
& MinMax & 0.312 & 0.883 & 0.458 \\
& Trim & 0.300 & 0.869 & 0.443 \\
\midrule
\multirow{4}{*}{Semantic} & Label-flip & 0.301 & 0.955 & 0.458 \\
& Backdoor & 1.000 & 0.897 & 0.946 \\
& Free-rider & 0.300 & 0.950 & 0.456 \\
& Collision & 1.000 & 0.743 & 0.847 \\
\midrule
\multicolumn{2}{@{}l}{Overall} & 0.349 & 0.899 & 0.503 \\
\bottomrule
\end{tabular}
\end{table}

FedACT achieves 89.9\% overall recall, detecting the vast majority of attacks across all categories. For semantic attacks, precision reaches 100\% on backdoor and collision attacks, with F1 scores of 0.946 and 0.847 respectively. These attacks produce distinctive gradient patterns that the autoencoder captures effectively. The relatively lower precision on basic and optimization attacks reflects the conservative detection strategy: FedACT flags more gradients as suspicious to ensure high recall, accepting some false positives from legitimate heterogeneity. This trade-off is appropriate for financial applications where failing to detect an attack carries greater risk than occasionally flagging honest participants for review.

\subsection{Comparison with Robust Aggregators}

Table~\ref{tab:accuracy} compares model accuracy across defense methods under representative attacks.

\begin{table}[h]
\centering
\caption{Model accuracy (\%) under attacks. Best per row in bold.}
\label{tab:accuracy}
\begin{tabular}{@{}lccccccc@{}}
\toprule
Attack & Median & Trim & Krum & M-Krum & Bulyan & RFA & FedACT \\
\midrule
Sign-flip & \textbf{84.75} & 84.76 & 84.78 & 84.97 & 84.71 & 84.71 & 83.48 \\
Gaussian & 84.82 & \textbf{84.90} & 84.71 & 84.85 & 84.74 & 84.75 & 83.56 \\
ALIE & 84.67 & 84.71 & 84.79 & \textbf{84.86} & 84.82 & 84.66 & 83.66 \\
Backdoor & 84.78 & 84.77 & \textbf{84.83} & \textbf{84.83} & 84.78 & 84.64 & 82.67 \\
Collision & 84.80 & \textbf{84.96} & 84.70 & 84.70 & 84.86 & 84.86 & 73.12 \\
\midrule
Average & 84.77 & \textbf{84.83} & 84.79 & 84.82 & 84.78 & 84.73 & 81.74 \\
\bottomrule
\end{tabular}
\end{table}

Traditional robust aggregators achieve approximately 3\% higher accuracy than FedACT. This gap reflects a fundamental design difference: robust aggregators silently filter any gradient that deviates from the majority, effectively treating all heterogeneity as noise to be suppressed. FedACT explicitly detects and classifies anomalies, maintaining records of which participants were flagged and why. The accuracy trade-off is the cost of providing detection capability.

Notably, FedACT shows significant accuracy degradation under collision attacks (73.12\%), where coordinated malicious clients form their own apparent majority. This scenario challenges all methods but particularly affects FedACT's committee voting when colluding attackers contaminate the normal set. Future work should address collusion-resistant committee selection.

\subsection{Heterogeneity Robustness}

Table~\ref{tab:heterogeneity} demonstrates stable detection performance across data heterogeneity scenarios.

\begin{table}[h]
\centering
\caption{Performance under heterogeneity scenarios.}
\label{tab:heterogeneity}
\begin{tabular}{@{}lcccc@{}}
\toprule
Scenario & Precision & Recall & F1 & Accuracy \\
\midrule
IID & 0.425 & 0.899 & 0.537 & 80.16 \\
Label skew & 0.434 & 0.899 & 0.544 & 83.36 \\
Feature skew & 0.437 & 0.905 & 0.546 & 81.91 \\
Quantity skew & 0.430 & 0.892 & 0.531 & 81.69 \\
\midrule
Average & 0.431 & 0.899 & 0.540 & 81.78 \\
\bottomrule
\end{tabular}
\end{table}

Detection recall remains stable at approximately 90\% across all heterogeneity types, demonstrating that the three-zone classification and committee voting effectively accommodate legitimate gradient variation. The slightly higher precision under heterogeneous scenarios compared to IID may seem counterintuitive but reflects that heterogeneity makes attacks more distinctive: when honest gradients vary naturally, malicious gradients that deviate in unusual ways become more identifiable.

\subsection{Ablation Study}

Table~\ref{tab:ablation} isolates the contribution of each component.

\begin{table}[h]
\centering
\caption{Ablation study results.}
\label{tab:ablation}
\begin{tabular}{@{}lcccc@{}}
\toprule
Configuration & Precision & Recall & F1 & Accuracy \\
\midrule
FedACT (full) & 0.349 & 0.899 & 0.503 & 81.78 \\
Without autoencoder & 0.285 & 0.712 & 0.407 & 82.45 \\
Without committee & 0.312 & 0.921 & 0.467 & 80.92 \\
Without TLBO & 0.349 & 0.899 & 0.503 & 80.45 \\
\midrule
FedAvg (no defense) & --- & --- & --- & 78.23 \\
\bottomrule
\end{tabular}
\end{table}

The autoencoder is essential: removing it reduces recall by 18.7 percentage points, demonstrating that learned representations capture attack patterns that simpler heuristics miss. The committee mechanism improves precision by 3.7 points while slightly reducing recall, confirming its role in filtering false positives from heterogeneity. TLBO aggregation improves accuracy by 1.3 points compared to simple averaging but does not affect detection metrics.


%% ============================================================================
%% CONCLUSION
%% ============================================================================
\section{Conclusion}
\label{sec:conclusion}

We present FedACT, a Byzantine-resilient federated learning framework that prioritizes explicit attack detection over implicit robustness. By combining autoencoder-based anomaly scoring, diversity-constrained committee voting, and reputation-weighted aggregation with Merkle-tree evidence logging, FedACT achieves 89.9\% detection recall with perfect precision on semantic attacks. While traditional robust aggregators achieve marginally higher model accuracy through silent outlier filtering, FedACT uniquely provides the detection capability and auditability essential for regulated financial environments.

The framework represents a deliberate trade-off appropriate for credit scoring applications where accountability matters. Regulatory compliance increasingly requires that automated decision systems maintain audit trails and provide explanations for their behavior. FedACT addresses this need by explicitly identifying anomalous participants, accumulating evidence through the reputation system, and maintaining tamper-evident records for auditors.

Limitations include vulnerability to sophisticated collusion attacks that form apparent majorities, and the inherent tension between detection sensitivity and false positive rates under extreme heterogeneity. Future work should explore collusion-resistant committee selection, adversarial training for autoencoder robustness, and formal convergence guarantees under Byzantine presence.


%% ============================================================================
%% ACKNOWLEDGMENTS
%% ============================================================================
\section*{Acknowledgments}

This work was supported by the National Natural Science Foundation of China (Grant No. 72171073).


%% ============================================================================
%% REFERENCES
%% ============================================================================
\bibliographystyle{elsarticle-num-names}
\bibliography{FedACT-refs}

\end{document}
